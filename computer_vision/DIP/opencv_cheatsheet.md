# OpenCV Cheat sheet

---

## Table 1: Basic Image I/O and Color Operations

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.imread(path, flag)` <br> Reads an image from the file specified by **path**, with **flag** determining the read mode (e.g., `cv2.IMREAD_COLOR`, `cv2.IMREAD_GRAYSCALE`). | Reads image from file into a NumPy array in BGR format (by default). | $I(x, y) = \text{pixel\_data\_from\_file}(x, y)$ | - Supports multiple formats.<br>- Simple to use. | - Path must exist.<br>- Default BGR format may require conversion. |
| `cv2.imwrite(filename, img)` <br> Saves an image **img** to the path **filename** in formats like JPEG, PNG. | Writes the NumPy array to the disk in the specified format. | *No numeric formula; encodes array into a file.* | - Supports various formats.<br>- Offers compression options. | - Can silently fail if the path is invalid. |
| `cv2.cvtColor(img, code)` <br> Converts **img** between color spaces using **code** (e.g., `cv2.COLOR_BGR2GRAY`, `cv2.COLOR_BGR2HSV`). | Converts image between color spaces (e.g., RGB to Grayscale or HSV). | RGB to Grayscale: $Y = 0.299R + 0.587G + 0.114B$<br>RGB to HSV: $V = \max(R, G, B)$<br>$S = \frac{V - \min(R, G, B)}{V}$ | - Offers standard conversions.<br>- Optimized performance. | - Limited to predefined color spaces.<br>- Potential loss during conversion. |
| `cv2.imshow(win_name, img)` <br> Displays an image **img** in a GUI window named **win_name**. | Visualizes an image in a pop-up window. | *No formula; simply displays the input image.* | - Useful for debugging and visualization.<br>- Quick to use. | - Requires a GUI environment.<br>- Blocks until a key press. |
| `cv2.waitKey(delay=0)` <br> Waits for a key press for up to **delay** milliseconds (0 waits indefinitely). | Enables capturing key input, often used with `cv2.imshow`. | *No numeric formula; pauses execution.* | - Necessary for image display in OpenCV.<br>- Can capture keyboard events. | - Blocks execution until delay expires or key is pressed. |
| `cv2.destroyAllWindows()` <br> Closes all OpenCV windows opened by `cv2.imshow`. | Releases GUI resources and ensures proper cleanup. | *No formula; simply closes GUI windows.* | - Prevents resource leakage.<br>- Easy to use. | - No effect if no windows are open.<br>- GUI-dependent. |
| `cv2.createTrackbar(trackbar_name, window_name, value, count, on_change)` <br> Creates a trackbar slider for real-time parameter adjustments. | Dynamically adjusts values (e.g., thresholds) during runtime. | Triggers the `on_change` callback when value changes. | - Interactive parameter tuning.<br>- Simplifies testing and debugging. | - Requires additional code for callbacks.<br>- Limited GUI layout customization. |
| `cv2.getTrackbarPos(trackbar_name, window_name)` <br> Retrieves the current value of a trackbar. | Accesses the current position (value) of a trackbar for dynamic use in code. | Returns the current slider value \(v\). | - Simple method to retrieve values.<br>- Works seamlessly with `cv2.createTrackbar`. | - Requires careful syncing between trackbar position and logic.<br>- Dependent on active GUI. |
| `img[y1:y2, x1:x2]` (Cropping) <br> Crops a region of interest (ROI) from an image by slicing rows from **y1:y2** and columns from **x1:x2**. | Extracts a subregion of the image for focused processing. | $\text{Cropped}(x, y) = I(x + x_1, y + y_1)$ <br> *for* $(0 \leq x < x_2 - x_1, 0 \leq y < y_2 - y_1)$ | - Intuitive slicing with NumPy.<br>- Zero extra memory if used as reference. | - Must ensure indices are within bounds.<br>- Does not resize automatically. |
| `cv2.split(img)` <br> Splits an image into its individual channels. | Decomposes a multi-channel image into separate single-channel arrays. | $\{I_B, I_G, I_R\} = \text{split}(I)$ <br> *where:* <br> $(I_B, I_G, I_R)$: Blue, Green, Red channels | - Simplifies channel-wise operations.<br>- Useful for color processing. | - Memory-intensive for large images.<br>- Requires recombining for visualization. |
| `cv2.merge(channels)` <br> Combines separate single-channel arrays into a multi-channel image. | Merges individual channels into a single image. | $I = \text{merge}(\{I_B, I_G, I_R\})$ <br> *where:* <br> $(I_B, I_G, I_R)$: Blue, Green, Red channels | - Combines processed channels easily.<br>- Supports flexible channel manipulation. | - Requires correct ordering of channels.<br>- Limited to compatible dimensions. |

---

## Table 2: Basic Arithmetic & Bitwise Operations

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.add(img1, img2)` <br> Adds pixel values of **img1** and **img2** with saturation at 255. | Element-wise addition of two images. | $I_{out}(x, y) = \min(I_1(x, y) + I_2(x, y), 255)$ <br> *where:* <br> $(I_1, I_2)$: Pixel values of images $(1)$ and $(2)$ | - Ensures no overflow.<br>- Useful for brightness adjustments. | - Requires both images to be the same size and type.<br>- Saturation can lose details. |
| `cv2.addWeighted(img1, w1, img2, w2, gamma)` <br> Blends **img1** and **img2** using weights **w1** and **w2** and scalar offset **gamma**. | Performs weighted blending of two images. | $I_{out}(x, y) = w_1 \cdot I_1(x, y) + w_2 \cdot I_2(x, y) + \gamma$ <br> *where:* <br> $(w_1, w_2)$: Weights for the images <br> $(\gamma)$: Scalar offset | - Smooth transitions for overlay effects.<br>- Supports transparency. | - Large weights may saturate.<br>- Images must match size and type. |
| `cv2.subtract(img1, img2)` <br> Subtracts pixel values of **img2** from **img1**, clamping at 0. | Element-wise subtraction of two images. | $I_{out}(x, y) = \max(I_1(x, y) - I_2(x, y), 0)$ <br> *where:* <br> $(I_1, I_2)$: Pixel values of images $(1)$ and $(2)$ | - Useful for background subtraction.<br>- Highlights differences. | - Negative differences are clipped.<br>- Requires same size and type for both images. |
| `cv2.bitwise_and(img1, img2, mask=mask)` <br> Performs a pixel-wise logical AND on **img1** and **img2**, optionally using a **mask**. | Combines two images or regions with logical AND. | $I_{out}(x, y) = I_1(x, y) \land I_2(x, y)$ <br> *where:* <br> $(\land)$: Logical AND operator | - Efficient for masking regions.<br>- Easy to segment regions. | - Requires same size and type.<br>- Masks must match dimensions. |
| `cv2.bitwise_or(img1, img2, mask=mask)` <br> Performs a pixel-wise logical OR on **img1** and **img2**, optionally using a **mask**. | Combines two images or regions with logical OR. | $I_{out}(x, y) = I_1(x, y) \lor I_2(x, y)$ <br> *where:* <br> $(\lor)$: Logical OR operator | - Easy to overlay regions.<br>- Efficient combination. | - Same size and type constraints.<br>- May include irrelevant details. |
| `cv2.bitwise_not(img, mask=mask)` <br> Performs a pixel-wise logical NOT on **img**, optionally using a **mask**. | Inverts the pixel values of an image. | $I_{out}(x, y) = \neg I(x, y)$ <br> *for 8-bit images:* $I_{out} = 255 - I$ <br> *where:* <br> $(\neg)$: Logical NOT operator | - Simple inversion effect.<br>- Useful for mask negation. | - Less common beyond special effects.<br>- Requires 8-bit or binary images. |
| `cv2.bitwise_xor(img1, img2, mask=mask)` <br> Performs a pixel-wise logical XOR on **img1** and **img2**, optionally using a **mask**. | Highlights differences between two images. | $I_{out}(x, y) = I_1(x, y) \oplus I_2(x, y)$ <br> *where:* <br> $(\oplus)$: Logical XOR operator | - Useful for emphasizing changes.<br>- Highlights differences. | - Requires same size and type.<br>- May produce unexpected results with noisy data. |

---

## Table 3: Image Processing - Filtering & Blurring

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.blur(img, ksize)` <br> Applies a normalized box filter (averaging filter) with kernel size **ksize**. | Smooths the image by averaging pixel values in a local neighborhood. | $I_{out}(x, y) = \frac{1}{k_x k_y} \sum_{u, v} I(x + u, y + v)$ <br> *where:* <br> $(k_x, k_y)$: Dimensions of the kernel <br> $(I(x, y))$: Input intensity | - Very fast and simple.<br>- Effective for mild smoothing. | - Blurs edges significantly.<br>- Not effective for salt-and-pepper noise. |
| `cv2.GaussianBlur(img, ksize, sigmaX)` <br> Applies Gaussian smoothing with kernel size **ksize** and standard deviation **sigmaX**. | Reduces noise while preserving edges using Gaussian smoothing. | $G(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}$ <br> *where:* <br> $(\sigma)$: Standard deviation <br> $(x, y)$: Pixel coordinates relative to kernel center | - Preserves edges.<br>- Common preprocessing. | - Slower than box filtering.<br>- Needs parameter tuning. |
| `cv2.filter2D(img, ddepth, kernel)` <br> Applies a general linear filter defined by **kernel** to the image. | Convolves the input image with a custom kernel. | $I_{out}(x, y) = \sum_{u, v} I(x + u, y + v) K(u, v)$ <br> *where:* <br> $(K)$: Convolution kernel <br> $(I(x, y))$: Input intensity | - Customizable filtering.<br>- Flexible for advanced filters. | - Computationally expensive for large kernels.<br>- Requires kernel design knowledge. |
| `cv2.getGaussianKernel(ksize, sigma)` <br> Generates a 1D Gaussian kernel of size **ksize** with standard deviation **sigma**. | Creates a separable Gaussian kernel for custom filtering. | $G(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{x^2}{2\sigma^2}}$ <br> *where:* <br> $(x)$: Coordinate along the kernel axis | - Generates kernels for custom usage.<br>- Supports separable filtering. | - Limited to 1D kernels.<br>- Requires manual application. |
| `cv2.sepFilter2D(img, ddepth, kernelX, kernelY)` <br> Applies separable 1D filters (e.g., Gaussian kernels) in X and Y directions. | Convolves the image using separable kernels for efficiency. | $I_{out}(x, y) = (I \ast K_x) \ast K_y$ <br> *where:* <br> $(K_x, K_y)$: Kernels for X and Y axes <br> $(\ast)$: Convolution operator | - Faster than 2D convolution.<br>- Preserves separable structure. | - Limited to separable kernels.<br>- Slightly less intuitive than `filter2D`. |
| `cv2.medianBlur(img, ksize)` <br> Replaces each pixel with the median value of its neighborhood defined by **ksize**. | Excellent for removing salt-and-pepper noise while preserving edges. | $I_{out}(x, y) = \text{median}\{I(x + u, y + v)\}$ <br> *where:* <br> $(u, v)$: Neighborhood offsets <br> $(I(x, y))$: Input intensity | - Preserves edges.<br>- Effective on impulse noise. | - Computationally slower.<br>- Less effective on Gaussian noise. |
| `cv2.bilateralFilter(img, d, sigmaColor, sigmaSpace)` <br> Smooths while preserving edges using a Gaussian filter in spatial and intensity domains. | Reduces noise while preserving strong edges. | $I_{out}(p) = \frac{1}{W_p} \sum_q e^{-\frac{\|p-q\|^2}{2\sigma_{space}^2}} e^{-\frac{\|I_p - I_q\|^2}{2\sigma_{color}^2}} I(q)$ <br> *where:* <br> $(p, q)$: Pixel locations <br> $(I_p, I_q)$: Intensities <br> $(W_p)$: Normalization factor | - Strong edge preservation.<br>- Smooths without blurring edges. | - Computationally expensive.<br>- Requires multiple parameters. |
| `cv2.copyMakeBorder(img, top, bottom, left, right, borderType, value)` <br> Adds a border to the image with custom padding widths and border type (e.g., `cv2.BORDER_CONSTANT`). | Extends the size of the image with specified border properties. | *No specific formula; depends on border type and padding values.* | - Flexible padding options.<br>- Useful for image preprocessing. | - Adds extra memory overhead.<br>- Requires manual parameter tuning. |

---

## Table 4: Thresholding & Segmentation

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.threshold(img, thresh, maxval, type)` <br> Applies global thresholding with fixed **thresh** and **maxval**, using **type** (e.g., `cv2.THRESH_BINARY`). | Converts grayscale to binary or semi-binary using a fixed threshold. | $T(x, y) = \begin{cases} \text{maxval} & \text{if } I(x, y) \geq \text{thresh} \\ 0 & \text{otherwise} \end{cases}$ <br> *where:* <br> $(I(x, y))$: Intensity of pixel $((x, y))$ | - Simple and fast.<br>- Easy to implement. | - Not adaptive to uneven lighting.<br>- Requires manual threshold selection. |
| `cv2.threshold(..., cv2.THRESH_OTSU)` <br> Computes an optimal threshold **thresh** automatically using Otsu's method. | Automatically determines a threshold by minimizing intra-class variance. | Minimizes: $J(t) = \sigma_B^2(t) = w_1(t) w_2(t) \left( \mu_1(t) - \mu_2(t) \right)^2$ <br> *where:* <br> $(w_1, w_2)$: Class probabilities <br> $(\mu_1, \mu_2)$: Class means | - Eliminates need for manual thresholding.<br>- Handles bimodal histograms well. | - Assumes bimodal distribution.<br>- Can fail on unimodal histograms. |
| `cv2.adaptiveThreshold(img, maxval, method, type, blockSize, C)` <br> Performs adaptive thresholding using **method** (e.g., `cv2.ADAPTIVE_THRESH_MEAN_C`) on blocks of size **blockSize**, with adjustment **C**. | Calculates threshold locally for smaller regions, useful for uneven illumination. | $T(x, y) = \text{mean/gaussian}(\text{neighborhood}) - C$ <br> *where:* <br> $(T(x, y))$: Local threshold <br> $(\text{mean/gaussian})$: Average or Gaussian-weighted intensity of the neighborhood | - Adapts to local variations in lighting.<br>- Effective for shadowed regions. | - Slower than global threshold.<br>- Requires careful tuning of parameters. |
| `cv2. (images, channels, hist, ranges, scale)` <br> Projects a histogram **hist** back onto an image based on the **channels** and **ranges**. | Measures how well pixel intensities in an image match the given histogram. | $B(x, y) = h(I(x, y))$ <br> *where:* <br> $h$: Histogram lookup function <br> $I(x, y)$: Pixel intensity at $(x, y)$ | - Useful for tracking objects based on histograms.<br>- Operates directly in pixel space. | - Sensitive to histogram quality.<br>- Requires preprocessing for good results. |
| `cv2.kmeans(data, K, None, criteria, attempts, flags)` <br> Groups data points into **K** clusters using the k-means algorithm. | Segments data (e.g., pixels) into **K** clusters by minimizing intra-cluster variance. | Minimizes: $J = \sum_{k=1}^{K} \sum_{x \in C_k} \|x - \mu_k\|^2$ <br> *where:* <br> $C_k$: Cluster $k$ <br> $\mu_k$: Centroid of cluster $k$ | - Effective for color quantization and segmentation.<br>- Easy to implement. | - Sensitive to initialization.<br>- May converge to local minima. |
| `cv2.inRange(img, lowerb, upperb)` <br> Checks if pixel values of **img** fall within the range defined by **lowerb** and **upperb**. Outputs a binary mask. | Creates a mask where pixels within the specified range are set to 255, and others to 0. | $M(x, y) = \begin{cases} 255 & \text{if } \text{lowerb} \leq I(x, y) \leq \text{upperb} \\ 0 & \text{otherwise} \end{cases}$ <br> *where:* <br> $(I(x, y))$: Intensity of pixel $((x, y))$ | - Handles range-based segmentation easily.<br>- Supports multi-channel images like HSV. | - Requires proper range selection.<br>- Sensitive to lighting and noise. |

---

### Thresholding Types (for `cv2.threshold`)

| **Type** | **Description** |
|----------|-----------------|
| `cv2.THRESH_BINARY` | Pixels are set to **maxval** if above threshold, otherwise 0. |
| `cv2.THRESH_BINARY_INV` | Inverts the binary thresholding: Pixels are set to 0 if above threshold, otherwise **maxval**. |
| `cv2.THRESH_TRUNC` | Pixels above the threshold are set to **thresh**, while others remain unchanged. |
| `cv2.THRESH_TOZERO` | Pixels below the threshold are set to 0, while others remain unchanged. |
| `cv2.THRESH_TOZERO_INV` | Inverts TOZERO thresholding: Pixels above the threshold are set to 0, while others remain unchanged. |

---

## Table 5: Morphological Operations

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.erode(img, kernel, iterations=1)` <br> Applies erosion using **kernel** for a specified number of **iterations**. | Removes small white noise and shrinks bright areas. | $I_{out} = I \ominus K$ <br> *where:* <br> $(I)$: Input image <br> $(K)$: Structuring element | - Removes noise.<br>- Separates connected objects. | - Shrinks regions.<br>- May lose small details. |
| `cv2.dilate(img, kernel, iterations=1)` <br> Applies dilation using **kernel** for a specified number of **iterations**. | Enlarges bright regions and fills small holes. | $I_{out} = I \oplus K$ <br> *where:* <br> $(I)$: Input image <br> $(K)$: Structuring element | - Fills holes.<br>- Joins disconnected parts. | - Enlarges regions.<br>- May merge features. |
| `cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iters)` <br> Performs opening (erosion followed by dilation) with **kernel** and **iters** iterations. | Removes small bright spots (noise) while preserving object shapes. | $\text{Open}(I) = (I \ominus K) \oplus K$ <br> *where:* <br> $(I)$: Input image <br> $(K)$: Structuring element | - Eliminates small foreground noise.<br>- Smooths boundaries. | - Removes small but valid features.<br>- Two-step process increases computation. |
| `cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iters)` <br> Performs closing (dilation followed by erosion) with **kernel** and **iters** iterations. | Fills small holes or gaps in bright regions. | $\text{Close}(I) = (I \oplus K) \ominus K$ <br> *where:* <br> $(I)$: Input image <br> $(K)$: Structuring element | - Closes small gaps.<br>- Smooths object boundaries. | - May connect objects that should remain separate.<br>- Two-step process increases computation. |
| `cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)` <br> Computes the morphological gradient (difference between dilation and erosion). | Highlights edges and transitions in the image. | $\text{Grad}(I) = (I \oplus K) - (I \ominus K)$ <br> *where:* <br> $(I)$: Input image <br> $(K)$: Structuring element | - Effective for edge detection.<br>- Simple operation. | - Less common for typical tasks.<br>- Sensitive to noise. |
| `cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)` <br> Computes the Top Hat transformation (original image minus its opening). | Highlights small bright regions or details. | $\text{TopHat}(I) = I - \text{Open}(I)$ <br> *where:* <br> $(I)$: Input image <br> $(\text{Open}(I))$: Result of opening | - Good for detecting small bright details.<br>- Effective for uneven illumination. | - May highlight irrelevant noise.<br>- Requires careful kernel selection. |
| `cv2.morphologyEx(img, op, kernel, iterations=1)` <br> Performs a general morphological transformation specified by **op** using **kernel** and a specified number of **iterations**. | Applies a morphological operation such as opening, closing, gradient, etc., on the input image. | Depends on **op**: <br> - Open: $(I \ominus K) \oplus K$ <br> - Close: $(I \oplus K) \ominus K$ <br> - Gradient: $(I \oplus K) - (I \ominus K)$ | - Flexible and powerful.<br>- Handles diverse morphological tasks. | - Requires proper **op** and **kernel** selection.<br>- Computational cost increases with iterations. |

---

### MorphologyEx Operation Types

| **Operation** | **Argument in `cv2.morphologyEx()`** | **Description** |
|---------------|--------------------------------------|------------------|
| Opening       | `cv2.MORPH_OPEN`                    | Erosion followed by dilation. Removes small bright spots or noise while preserving object shapes. |
| Closing       | `cv2.MORPH_CLOSE`                   | Dilation followed by erosion. Fills small holes or gaps in bright regions. |
| Gradient      | `cv2.MORPH_GRADIENT`                | Difference between dilation and erosion. Highlights edges and transitions. |
| Top Hat       | `cv2.MORPH_TOPHAT`                  | Original image minus its opening. Highlights small bright regions or details. |
| Black Hat     | `cv2.MORPH_BLACKHAT`                | Closing minus the original image. Highlights small dark regions on a bright background. |

---

## Table 6: Geometric Transformations

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.resize(img, dsize, fx=None, fy=None, interpolation=cv2.INTER_LINEAR)` <br> Resizes **img** to dimensions **dsize** or scales it by **fx** and **fy** using the specified **interpolation** method. | Changes image size, either by scaling or specifying an output size. | Depends on interpolation method: <br> Nearest neighbor, bilinear, cubic, etc. | - Flexible resizing.<br>- Supports multiple interpolation methods. | - Artifacts may appear with poor interpolation.<br>- Tradeoff between quality and speed. |
| `cv2.warpAffine(img, M, dsize, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)` <br> Applies an affine transformation using the 2×3 transformation matrix **M**. | Transforms the image while preserving straight lines and parallelism. | $\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}$ | - Preserves object shapes.<br>- Efficient for geometric transformations. | - Limited to affine transformations.<br>- Interpolation may blur details. |
| `cv2.warpPerspective(img, M, dsize, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)` <br> Applies a perspective transformation using the 3×3 transformation matrix **M**. | Transforms the image for perspective projection. | $\begin{bmatrix} x' \\ y' \\ w \end{bmatrix} = \begin{bmatrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33} \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}, \quad \text{and: } x' = \frac{x'}{w}, y' = \frac{y'}{w}$ | - Handles projective transformations.<br>- Useful for correcting perspective distortions. | - More computationally intensive than affine.<br>- Sensitive to matrix accuracy. |
| `cv2.getAffineTransform(src, dst)` <br> Computes a 2×3 affine transformation matrix from three corresponding points in **src** and **dst**. | Calculates a matrix for affine transformations like scaling, rotation, and translation. | Solves for $M$ such that $\begin{bmatrix} x' \\ y' \end{bmatrix} = M \cdot \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$ | - Easy to calculate transformations.<br>- Supports basic geometric adjustments. | - Limited to linear transformations.<br>- Cannot handle perspective changes. |
| `cv2.getPerspectiveTransform(src, dst)` <br> Computes a 3×3 perspective transformation matrix from four corresponding points in **src** and **dst**. | Calculates the matrix for perspective transformations like warping or correcting distortions. | Solves for $M$ such that $\begin{bmatrix} x' \\ y' \\ w \end{bmatrix} = M \cdot \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$ | - Allows advanced projective changes.<br>- Useful for real-world warping corrections. | - Requires accurate source and destination points.<br>- Computationally more expensive. |
| `cv2.getRotationMatrix2D(center, angle, scale)` <br> Creates a 2×3 transformation matrix for rotating **angle** degrees around **center** and scaling by **scale**. | Rotates the image while optionally scaling it. | $M = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} \cdot \text{Scale}$ <br> *where:* <br> $(\theta)$: Rotation angle | - Simple and efficient for rotations.<br>- Combines rotation and scaling. | - Limited to in-plane rotations.<br>- May cause clipping at image edges. |
| `cv2.remap(img, map1, map2, interpolation, borderMode=cv2.BORDER_CONSTANT)` <br> Maps pixels from the source **img** to a new location defined by **map1** and **map2** using **interpolation**. | General pixel-wise remapping of an image. | New coordinates: $(x', y') = (map1(x, y), map2(x, y))$ | - Allows arbitrary pixel transformations.<br>- Supports distortion corrections. | - Requires precomputed mapping.<br>- Memory-intensive for large images. |
| `cv2.findHomography(srcPoints, dstPoints, method=cv2.RANSAC, ransacReprojThreshold=3.0)` <br> Computes a homography matrix to map points in **srcPoints** to **dstPoints**. | Maps corresponding points using a perspective transformation. | Finds $H$ such that $\begin{bmatrix} x'_i \\ y'_i \\ w'_i \end{bmatrix} = H \cdot \begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}, \quad \text{and: } x' = \frac{x'_i}{w'_i}, y' = \frac{y'_i}{w'_i}$ | - Robust to outliers when using RANSAC.<br>- Useful for alignment and stitching. | - Sensitive to noise if not using RANSAC.<br>- Requires sufficient point correspondences. |

---

## Table 7: Contours & Shape Analysis

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.findContours(img, mode, method)` <br> Finds contours in binary **img** using **mode** for retrieval (e.g., `cv2.RETR_TREE`) and **method** for approximation (e.g., `cv2.CHAIN_APPROX_SIMPLE`). | Identifies and retrieves contours of objects in a binary image. | Boundary following algorithm to extract object outlines. | - Useful for shape analysis.<br>- Supports hierarchy. | - Requires a binary image.<br>- Sensitive to noise. |
| `cv2.contourArea(contour)` <br> Computes the area of a **contour**. | Calculates the area enclosed by a contour. | $A = \frac{1}{2} \sum_{i=0}^{n-1} (x_i y_{i+1} - x_{i+1} y_i)$ <br> *where:* <br> $(x_i, y_i)$: Points in the contour | - Simple computation.<br>- Accurate for closed contours. | - Works only for 2D contours.<br>- Assumes closed shapes. |
| `cv2.arcLength(contour, closed)` <br> Calculates the perimeter of a **contour**, with **closed** indicating whether the contour is closed. | Computes the sum of the Euclidean distances between consecutive contour points. | $\text{Perimeter} = \sum_{i=0}^{n-1} \sqrt{(x_{i+1} - x_i)^2 + (y_{i+1} - y_i)^2}$ <br> *where:* <br> $(x_i, y_i)$: Contour points | - Simple calculation.<br>- Optional closure. | - Discrete approximation.<br>- 2D only. |
| `cv2.moments(contour, binaryImage=False)` <br> Calculates spatial and central moments of a **contour** or **binaryImage**. | Computes moments for shape analysis and feature extraction. | Spatial moments: $m_{pq} = \sum_x \sum_y x^p y^q I(x, y)$ <br> Central moments: $\mu_{pq} = \sum_x \sum_y (x - \bar{x})^p (y - \bar{y})^q I(x, y)$ <br> *where:* <br> $(\bar{x} = \frac{m_{10}}{m_{00}}, \bar{y} = \frac{m_{01}}{m_{00}})$ | - Essential for centroid, orientation, and shape descriptors.<br>- Useful for shape matching. | - Requires accurate contour or binary image.<br>- Sensitive to noise in input data. |
| `cv2.approxPolyDP(curve, epsilon, closed)` <br> Approximates a polygonal curve for **curve** with tolerance **epsilon**. The **closed** parameter specifies if the curve is closed. | Reduces the number of points in a contour while maintaining its shape. | Finds approximate vertices such that: <br> $\|d(i)\| \leq \epsilon$ <br> *where:* <br> $(d(i))$: Perpendicular distance from contour point $(i)$ to the polygon. | - Reduces complexity for contour processing.<br>- Useful for shape simplification. | - May lose small details.<br>- Sensitive to epsilon value. |
| `cv2.pointPolygonTest(contour, point, measureDist)` <br> Tests the relationship of a **point** with respect to a **contour**. Returns 1 if inside, -1 if outside, and 0 if on the contour. If **measureDist** is `True`, it also returns the signed distance from the point to the contour. | Determines whether a point is inside, outside, or on a contour. | Computes the shortest Euclidean distance: <br> $d(p, C) = \min \|p - c\|$ <br> *where:* <br> $(p)$: Query point <br> $(C)$: Contour <br> $(\| \cdot \|)$: Euclidean norm | - Useful for hit-testing and distance measurements.<br>- Works with any contour. | - Requires well-defined contours.<br>- Sensitive to contour resolution. |

Here are the two separate tables:

---

### Table: Contour Retrieval Modes

| **Mode**                  | **Argument in `cv2.findContours()`** | **Description** |
|---------------------------|--------------------------------------|------------------|
| External Contours         | `cv2.RETR_EXTERNAL`                 | Retrieves only the outermost contours. All child contours are ignored. |
| List of All Contours      | `cv2.RETR_LIST`                     | Retrieves all contours without establishing any hierarchical relationships. |
| Two-Level Hierarchy       | `cv2.RETR_CCOMP`                    | Retrieves all contours and organizes them into a two-level hierarchy: external contours and their child contours. |
| Full Hierarchy of Contours| `cv2.RETR_TREE`                     | Retrieves all contours and organizes them into a full hierarchy of nested contours. |

---

### Table: Contour Approximation Methods

| **Method**                | **Argument in `cv2.findContours()`** | **Description** |
|---------------------------|--------------------------------------|------------------|
| No Compression            | `cv2.CHAIN_APPROX_NONE`             | Stores all the contour points without any compression. Results in the highest memory usage. |
| Simple Compression         | `cv2.CHAIN_APPROX_SIMPLE`           | Compresses horizontal, vertical, and diagonal segments and only stores their end points. Reduces memory usage significantly. |
| Teh-Chin (L1 Distance)     | `cv2.CHAIN_APPROX_TC89_L1`          | Applies the Teh-Chin chain approximation algorithm (L1 distance metric) for contour representation. |
| Teh-Chin (K-Cosine Metric) | `cv2.CHAIN_APPROX_TC89_KCOS`        | Uses the Teh-Chin chain approximation algorithm (k-cosine metric) for contour representation. |

---

## Table 8: Gradient-Based Edge Detection and Differential Operators

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.Sobel(img, ddepth, dx, dy, ksize)` <br> Computes first derivative in **dx** or **dy** direction using a Sobel kernel of size **ksize**. | Detects edges by calculating image gradients. | $G_x = \frac{\partial I}{\partial x}, \quad G_y = \frac{\partial I}{\partial y}$ | - Effective for detecting edges.<br>- Distinguishes horizontal/vertical edges. | - Sensitive to noise.<br>- Requires gradient combination for edge magnitude. |
| `cv2.spatialGradient(img, ksize)` <br> Computes both **dx** and **dy** gradients simultaneously using Sobel operators. | Produces the full gradient field of the image. | $\nabla I = \left( \frac{\partial I}{\partial x}, \frac{\partial I}{\partial y} \right)$ | - Simultaneously computes gradients.<br>- Reduces computation time. | - Same limitations as Sobel.<br>- Limited to fixed Sobel operators. |
| `cv2.magnitude(dx, dy)` <br> Computes the magnitude of gradients from **dx** and **dy**. | Calculates the gradient strength. | $\|G\| = \sqrt{G_x^2 + G_y^2}$ | - Simplifies gradient magnitude computation.<br>- Integrates seamlessly with Sobel. | - Sensitive to noise.<br>- Requires precomputed gradients. |
| `cv2.cartToPolar(dx, dy)` <br> Converts Cartesian gradients to polar representation (magnitude and direction). | Provides gradient magnitude and direction. | $\|G\| = \sqrt{G_x^2 + G_y^2}, \quad \theta = \tan^{-1}\left(\frac{G_y}{G_x}\right)$ | - Captures both edge strength and orientation.<br>- Works well for directional analysis. | - Requires precomputed gradients.<br>- Can be computationally intensive. |
| `cv2.Laplacian(img, ddepth, ksize)` <br> Computes the Laplacian (2nd derivative) for all-direction edge detection. | Highlights regions of rapid intensity change. | $\nabla^2 I = \frac{\partial^2 I}{\partial x^2} + \frac{\partial^2 I}{\partial y^2}$ | - Detects edges in all directions.<br>- Enhances image features. | - Amplifies noise significantly.<br>- Computationally expensive. |
| `cv2.Canny(img, t_lower, t_upper, apertureSize, L2gradient)` <br> Applies Canny edge detection using thresholds **t_lower**, **t_upper**. | Finds edges using gradient magnitude, non-max suppression, and hysteresis thresholding. | 1. Gaussian Blur <br> 2. Gradient Magnitude: $\sqrt{G_x^2 + G_y^2}$ <br> 3. Thresholds and Edge Tracing | - Produces clean edge maps.<br>- Reduces false positives. | - Requires parameter tuning.<br>- More complex than Sobel/Laplacian. |

---

## Table 9: Hough Transforms (Line & Circle Detection)

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.HoughLines(edges, rho, theta, threshold)` <br> Detects lines in **edges** using distance resolution **rho** and angle resolution **theta**. | Detects infinite lines in edge-detected images using the Hough Transform. | $\rho = x \cos(\theta) + y \sin(\theta)$ | - Robust line detection.<br>- Handles noisy images. | - Produces infinite lines.<br>- Requires high threshold tuning. |
| `cv2.HoughLinesP(edges, rho, theta, threshold, minLineLength, maxLineGap)` <br> Probabilistic version of Hough Transform for line detection. | Detects finite line segments using additional constraints. | Same as `cv2.HoughLines` but considers segment constraints. | - Directly gives finite line segments.<br>- Fewer false positives. | - Requires additional tuning (e.g., `minLineLength`, `maxLineGap`). |
| `cv2.HoughCircles(img, method, dp, minDist, param1, param2, minRadius, maxRadius)` <br> Detects circles in grayscale **img** using the Hough Transform. | Finds circles by voting in a parametric space. | $(x - x_c)^2 + (y - y_c)^2 = r^2$ <br> *where:* <br> $(x_c, y_c)$: Circle center <br> $(r)$: Radius | - Detects circles of varying radii.<br>- Handles partial circles. | - Sensitive to noise.<br>- Parameter tuning needed. |

---

## Table 10: Statistical & Histogram Operations

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.calcHist([img], channels, mask, histSize, ranges)` <br> Computes the histogram of **img** over specified **channels** with optional **mask**. | Computes intensity or color distribution for an image. | $H(i) = \sum_{x, y} \delta(I(x, y) - i)$ <br> *where:* <br> $(H(i))$: Histogram value for bin $(i)$ <br> $(I(x, y))$: Intensity at pixel $(x, y)$ | - Flexible binning.<br>- Works with masks. | - Memory intensive.<br>- Parameter tuning. |
| `cv2.equalizeHist(img)` <br> Equalizes the histogram of a grayscale **img**. | Enhances image contrast by redistributing intensity values. | $h(v) = \text{round} \left( \frac{cdf(v) - cdf_{min}}{(M \times N) - cdf_{min}} \times (L - 1) \right)$ <br> *where:* <br> $(cdf(v))$: Cumulative distribution function <br> $(M, N)$: Image dimensions <br> $(L)$: Number of intensity levels | - Enhances low-contrast images.<br>- Simple to apply. | - Global operation.<br>- Can over-enhance. |
| `cv2.matchTemplate(img, templ, method)` <br> Matches template **templ** to regions in **img** using specified **method**. | Measures similarity of a template in an image. | For `SQDIFF`: $R(x, y) = \sum_{x', y'} (T(x', y') - I(x + x', y + y'))^2$ <br> *where:* <br> $(T(x', y'))$: Template intensity <br> $(I(x', y'))$: Image intensity | - Simple and effective.<br>- Multiple comparison methods. | - Sensitive to scale/rotation.<br>- Computationally expensive. |

---

## Table 11: Frequency Domain Processing

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.dft(img, flags)` <br> Computes the Discrete Fourier Transform (DFT) of **img**. | Analyzes image frequency components. | $F(u, v) = \sum_{x, y} f(x, y) e^{-j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}$ <br> *where:* <br> $(M, N)$: Image dimensions <br> $(f(x, y))$: Intensity at $((x, y))$ | - Enables frequency analysis.<br>- Useful for filtering. | - Complex output.<br>- Computationally intensive. |
| `cv2.idft(img, flags)` <br> Computes the inverse DFT to reconstruct an image. | Reconstructs spatial domain image from frequency domain. | $f(x, y) = \frac{1}{MN} \sum_{u, v} F(u, v) e^{j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}$ <br> *where:* <br> $(M, N)$: Image dimensions <br> $(F(u, v))$: Frequency coefficient | - Perfect recovery.<br>- Useful for filtering pipelines. | - Requires complex input.<br>- Memory intensive. |

---

## Table 12: Drawing Shapes & Text

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.line(img, start, end, color, thickness)` <br> Draws a line from **start** to **end** on **img** with specified **color** and **thickness**. | Creates a straight line segment between two points. | Line equation: $(x_1, y_1) \to (x_2, y_2)$ <br> *where:* <br> $(x_1, y_1)$: Starting coordinates <br> $(x_2, y_2)$: Ending coordinates | - Simple annotation tool.<br>- Useful for marking specific locations. | - Limited to straight lines.<br>- No antialiasing in low resolutions. |
| `cv2.rectangle(img, pt1, pt2, color, thickness)` <br> Draws a rectangle defined by corners **pt1** and **pt2**. | Creates a rectangle or bounding box. | Rectangle vertices: $(x_1, y_1), (x_2, y_1), (x_1, y_2), (x_2, y_2)$ <br> *where:* <br> $(x_1, y_1)$: Top-left corner <br> $(x_2, y_2)$: Bottom-right corner | - Useful for bounding boxes.<br>- Versatile shape. | - Filled rectangle may obscure details.<br>- Requires precise corner coordinates. |
| `cv2.circle(img, center, radius, color, thickness)` <br> Draws a circle with **center**, **radius**, **color**, and **thickness**. | Creates a circular region or outline. | Circle equation: $(x - x_c)^2 + (y - y_c)^2 = r^2$ <br> *where:* <br> $(x_c, y_c)$: Center coordinates <br> $(r)$: Radius | - Great for highlighting circular ROIs.<br>- Quick and efficient. | - Filled circle may block details.<br>- Limited to circular shapes. |
| `cv2.polylines(img, pts, isClosed, color, thickness)` <br> Draws connected line segments defined by points in **pts**, optionally **isClosed**. | Creates polygons by connecting points. | Sequence of line segments: $(x_i, y_i) \to (x_{i+1}, y_{i+1})$ <br> *where:* <br> $((x_i, y_i))$: Consecutive points in $(pts)$ | - Versatile for arbitrary shapes.<br>- Can create both open and closed shapes. | - Complex shapes require many points.<br>- Manual coordination of points needed. |
| `cv2.putText(img, text, org, fontFace, fontScale, color, thickness, lineType)` <br> Draws **text** on **img** at **org** using font **fontFace**, size **fontScale**, and **color**. | Renders text for labeling or annotation. | Renders text string at bottom-left corner $(x, y)$ <br> *where:* <br> $(x, y)$: Coordinates of the text's baseline | - Useful for adding labels or metadata.<br>- Customizable font size and thickness. | - Limited font styles.<br>- Can be hard to align for complex layouts. |
| `cv2.drawContours(img, contours, contourIdx, color, thickness, lineType=cv2.LINE_8)` <br> Draws **contours** on **img**, using specified **color** and **thickness**. **contourIdx** specifies which contour to draw (`-1` for all). | Visualizes contour outlines on the image. | Draws boundary points of contour $C$: $(x_1, y_1), (x_2, y_2), \dots$ | - Excellent for highlighting object boundaries.<br>- Supports hierarchical contour drawing. | - Requires precomputed contours.<br>- Limited control over detail level. |
| `cv2.ellipse(img, center, axes, angle, startAngle, endAngle, color, thickness)` <br> Draws an ellipse centered at **center** with **axes**, rotated by **angle**, starting from **startAngle** to **endAngle**. | Creates a full or partial ellipse. | Ellipse equation: $\frac{(x - x_c)^2}{a^2} + \frac{(y - y_c)^2}{b^2} = 1$ <br> *where:* <br> $(x_c, y_c)$: Center <br> $(a, b)$: Semi-major and semi-minor axes | - Flexible for both full and partial ellipses.<br>- Useful for shape annotations. | - Requires precise axis and angle values.<br>- Filled ellipses may obscure details. |
| `cv2.fillPoly(img, pts, color, lineType=cv2.LINE_8)` <br> Fills a polygon defined by points in **pts** with the specified **color**. | Creates filled polygons, useful for masking or regions of interest. | Fills all pixels enclosed within the polygon $P$. | - Excellent for creating masks.<br>- Efficient for large areas. | - Requires careful point specification.<br>- Cannot handle overlapping regions without preprocessing. |
| `cv2.fillConvexPoly(img, pts, color, lineType=cv2.LINE_8)` <br> Fills a convex polygon defined by points in **pts** with the specified **color**. | Efficiently fills convex polygons. | Same as `fillPoly`, optimized for convex polygons. | - Faster for convex shapes.<br>- No need to check for concavity. | - Limited to convex polygons.<br>- Requires preprocessing for concave polygons. |

---

## Table 13: Feature Detection and Corner Detection

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.goodFeaturesToTrack(img, maxCorners, qualityLevel, minDistance)` <br> Detects corners using Shi-Tomasi corner detection algorithm on **img**. Limits the number of corners to **maxCorners**, with **qualityLevel** controlling corner quality, and **minDistance** ensuring spacing between corners. | Identifies strong corners for feature tracking or alignment. | Uses Shi-Tomasi method: $R = \min(\lambda_1, \lambda_2)$ <br> *where:* <br> $(\lambda_1, \lambda_2)$: Eigenvalues of the image gradient matrix. | - Efficient and robust.<br>- Finds corners suitable for tracking. | - Sensitive to noise.<br>- Requires grayscale input. |
| `cv2.cornerHarris(img, blockSize, ksize, k)` <br> Computes Harris corner response on **img** using **blockSize** (neighborhood size), **ksize** (Sobel kernel size), and **k** (Harris detector free parameter). | Detects corners by analyzing changes in pixel intensity gradients. | $R = \det(M) - k(\text{trace}(M))^2$ <br> *where:* <br> $(M = \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix})$: Second moment matrix of gradients. | - Robust to rotation.<br>- Simple and effective for corner detection. | - Sensitive to parameter tuning.<br>- May detect noise as corners. |
| `cv2.SIFT_create()` <br> (Use `detectAndCompute` with SIFT) Detects keypoints and computes descriptors using the Scale-Invariant Feature Transform (SIFT) algorithm. | Identifies scale and rotation-invariant keypoints and their descriptors. | Keypoint detection uses difference-of-Gaussians: $D(x, y, \sigma) = L(x, y, k\sigma) - L(x, y, \sigma)$ <br> Descriptor: Local gradient histograms. | - Scale and rotation-invariant.<br>- Robust to changes in lighting. | - Computationally intensive.<br>- Requires careful threshold tuning. |
| `cv2.ORB_create()` <br> Detects and describes keypoints using the Oriented FAST and Rotated BRIEF (ORB) method. | Efficient and fast keypoint detection and descriptor extraction. | Combines FAST for corner detection and BRIEF for descriptors. <br> Orientation is determined using intensity-weighted centroid. | - Fast and efficient.<br>- Open-source alternative to SIFT. | - Not invariant to significant scale changes.<br>- May fail with high-frequency textures. |
| `cv2.FAST.detect(img, mask=None)` <br> Detects keypoints using the Features from Accelerated Segment Test (FAST) algorithm. | Identifies keypoints by comparing pixel intensities around a circle. | Pixel $(p)$ is a corner if there are \(n\) contiguous pixels on a circle brighter or darker than \(p\): $|I(p) - I(c)| > t$ <br> *where:* <br> $(c)$: Neighboring pixels on the circle <br> $(t)$: Intensity threshold. | - Extremely fast.<br>- Suitable for real-time applications. | - No orientation or scale information.<br>- Sensitive to noise. |
| `cv2.BRISK_create()` <br> Detects keypoints and computes binary descriptors using the Binary Robust Invariant Scalable Keypoints (BRISK) method. | Efficient detection of keypoints with binary descriptors. | Keypoints are detected using AGAST and described with binary comparisons of smoothed image intensities. | - Fast and efficient.<br>- Rotationally invariant. | - Less robust to scale changes.<br>- Descriptor performance depends on lighting. |
| `cv2.AKAZE_create()` <br> Detects and describes keypoints using the Accelerated-KAZE algorithm. | Extracts keypoints and descriptors efficiently using nonlinear scale spaces. | Nonlinear diffusion for scale space creation: $L(x, y, \sigma) = G(x, y, \sigma) \ast I(x, y)$ | - Scale and rotation-invariant.<br>- Faster than KAZE. | - Limited descriptor size.<br>- Less robust for very large-scale changes. |
| `cv2.KAZE_create()` <br> Detects and describes keypoints using KAZE with nonlinear diffusion. | Identifies features in nonlinear scale space for improved accuracy. | Uses nonlinear scale space with edge-preserving filters: $D_t = c(x, y, t) \Delta I(x, y, t)$ | - High accuracy.<br>- Robust to noise and transformations. | - Slower than AKAZE.<br>- Computationally expensive. |

---

## Table 14: Template Matching

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.matchTemplate(img, templ, method)` <br> Matches template **templ** within **img** using the specified **method** (e.g., `cv2.TM_CCOEFF`, `cv2.TM_SQDIFF`). | Identifies regions in the image that match the template. | For `TM_SQDIFF`: $R(x, y) = \sum_{x', y'} (T(x', y') - I(x + x', y + y'))^2$ <br> For `TM_CCOEFF`: $R(x, y) = \frac{\sum_{x', y'} (T(x', y') - \bar{T})(I(x + x', y + y') - \bar{I})}{\sqrt{\sum_{x', y'} (T(x', y') - \bar{T})^2 \sum_{x', y'} (I(x + x', y + y') - \bar{I})^2}}$ <br> *where:* <br> $(T(x', y'))$: Template pixel value <br> $(I(x, y))$: Image pixel value <br> $(\bar{T}, \bar{I})$: Mean values of template and region in the image. | - Easy to implement.<br>- Supports multiple similarity measures.<br>- Efficient for small templates. | - Sensitive to scale and rotation changes.<br>- Computationally expensive for large templates.<br>- Prone to false matches in repetitive patterns. |
| `cv2.minMaxLoc(result)` <br> Finds the global minimum and maximum locations in the result matrix from `cv2.matchTemplate`. | Identifies the best matching location depending on the method used. | For `TM_SQDIFF`, the minimum value indicates the best match.<br>For other methods, the maximum value indicates the best match. | - Easy to identify the most probable match.<br>- Complements template matching. | - Depends on template matching limitations.<br>- Requires careful interpretation of results. |

### Template Matching Methods

| **Method**             | **Description**                                                                                           |
|-------------------------|-----------------------------------------------------------------------------------------------------------|
| `cv2.TM_SQDIFF`        | Normalized squared differences. Lower values indicate a better match.                                     |
| `cv2.TM_SQDIFF_NORMED` | Squared differences normalized by image energy. Lower values indicate a better match.                     |
| `cv2.TM_CCORR`         | Cross-correlation. Higher values indicate a better match.                                                |
| `cv2.TM_CCORR_NORMED`  | Normalized cross-correlation. Higher values indicate a better match.                                      |
| `cv2.TM_CCOEFF`        | Correlation coefficient between the template and image patch. Higher values indicate a better match.      |
| `cv2.TM_CCOEFF_NORMED` | Normalized correlation coefficient. Higher values indicate a better match.                                |

---

## Table 15: Image Pyramids

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.pyrDown(img, dstsize=None, borderType=cv2.BORDER_DEFAULT)` <br> Reduces the size of **img** to half using Gaussian filtering. | Creates a lower-resolution version of the image. | $I_{\text{down}}(x, y) = \frac{1}{16} \left( I(2x, 2y) + 4I(2x+1, 2y) + 6I(2x+1, 2y+1) + \dots \right)$ <br> *where:* <br> $(I(x, y))$: Input image intensity | - Reduces resolution effectively.<br>- Smoothes the image during reduction. | - Smoothing can blur details.<br>- May cause aliasing artifacts. |
| `cv2.pyrUp(img, dstsize=None, borderType=cv2.BORDER_DEFAULT)` <br> Increases the size of **img** to double using Gaussian filtering. | Creates a higher-resolution version of the image. | $I_{\text{up}}(x, y) = \sum_{u,v} G(u,v) \cdot I\left(\frac{x-u}{2}, \frac{y-v}{2}\right)$ <br> *where:* <br> $(G(u, v))$: Gaussian kernel <br> $(I(x, y))$: Input image intensity | - Smoothly interpolates new pixels.<br>- Useful for blending or reconstruction. | - Interpolated regions may lack detail.<br>- Increased size increases memory usage. |
| `cv2.buildPyramid(img, maxlevel)` <br> Generates a Gaussian pyramid of **maxlevel** levels for **img**. | Constructs a sequence of images with successively reduced resolutions. | Iteratively applies `cv2.pyrDown` to create the pyramid. | - Useful for multiscale processing.<br>- Common in blending and object detection. | - Computationally expensive for high levels.<br>- Detail loss in lower levels. |
| **Laplacian Pyramid** (constructed manually using `cv2.pyrUp` and `cv2.subtract`) | Represents the difference between an image and its Gaussian approximation at each level. | $L_{\text{level}} = I_{\text{level}} - \text{pyrUp}(I_{\text{level+1}})$ | - Useful for reconstructing images.<br>- Captures edge and texture details. | - Requires manual implementation.<br>- Memory-intensive for multiple levels. |

---

## Table 16: Frequency Domain Processing

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT)` <br> Computes the Discrete Fourier Transform (DFT) of **img**. Use **flags** to control output (e.g., `cv2.DFT_COMPLEX_OUTPUT`). | Converts the image from the spatial domain to the frequency domain. | $F(u, v) = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x, y) e^{-j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}$ <br> *where:* <br> $(f(x, y))$: Intensity at pixel $((x, y))$ <br> $(M, N)$: Image dimensions | - Useful for frequency-based filtering.<br>- Captures periodic patterns. | - Produces complex output.<br>- Computationally expensive for large images. |
| `cv2.idft(img, flags=cv2.DFT_REAL_OUTPUT)` <br> Computes the Inverse Discrete Fourier Transform (IDFT) to reconstruct **img** from the frequency domain. | Converts data back to the spatial domain. | $f(x, y) = \frac{1}{MN} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u, v) e^{j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}$ <br> *where:* <br> \(F(u, v)\): Frequency domain coefficient <br> \(M, N\): Image dimensions | - Perfect recovery of the original image.<br>- Used to process filtered frequency data. | - Requires proper scaling.<br>- Complex input may require interpretation. |
| `np.fft.fft2(img)` <br> Computes the 2D Fourier Transform of **img** using NumPy. | Alternative to `cv2.dft` for transforming an image into the frequency domain. | Same as DFT formula. | - Easy to use.<br>- Standardized NumPy implementation. | - Outputs complex results.<br>- Not optimized for OpenCV pipelines. |
| `np.fft.ifft2(img)` <br> Computes the inverse 2D Fourier Transform of **img** using NumPy. | Reconstructs the image from frequency components. | Same as IDFT formula. | - Easy to use.<br>- Complements NumPy-based workflows. | - Not optimized for large-scale processing. |
| `cv2.magnitude(complex_img[:,:,0], complex_img[:,:,1])` <br> Computes the magnitude of the complex Fourier coefficients from the real and imaginary parts. | Converts Fourier coefficients to their magnitude representation for visualization. | $\text{Magnitude}(u, v) = \sqrt{\text{Real}(u, v)^2 + \text{Imaginary}(u, v)^2}$ | - Visualizes frequency domain data.<br>- Simplifies interpretation of DFT results. | - Loses phase information.<br>- Requires proper scaling for display. |
| `cv2.phase(real, imag, angleInDegrees=False)` <br> Computes the phase angle from real and imaginary components. | Extracts the phase information of Fourier coefficients. | $\text{Phase}(u, v) = \tan^{-1}\left(\frac{\text{Imaginary}(u, v)}{\text{Real}(u, v)}\right)$ | - Useful for phase-based analysis.<br>- Complements magnitude data. | - Requires handling of complex data.<br>- Interpretation may be non-intuitive. |
| `cv2.getOptimalDFTSize(size)` <br> Calculates an optimal size for the DFT computation to improve efficiency. | Determines the optimal size for efficient Fourier transforms. | Returns the nearest size that is a multiple of 2, 3, or 5. | - Optimizes performance for large images.<br>- Reduces computation time. | - Only applies to the DFT size.<br>- Requires precomputation. |

---

## Table 17: Connected Components Analysis

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.connectedComponents(binaryImage, connectivity=8)` <br> Labels connected components in a **binaryImage** using the specified **connectivity** (4 or 8). | Assigns unique labels to each connected region in a binary image. | Identifies regions $(L(x, y))$: <br> $L(x, y) = \begin{cases} k & \text{if connected to region } k \\ 0 & \text{otherwise} \end{cases}$ <br> *where:* <br> $(k)$: Label of the connected region. | - Simple and efficient.<br>- Provides region labeling. | - Only works for binary images.<br>- Sensitive to thresholding noise. |
| `cv2.connectedComponentsWithStats(binaryImage, connectivity=8)` <br> Extends `cv2.connectedComponents` to also compute statistics and centroids for each component. | Returns bounding box, area, and centroid for each connected component. | For each labeled region $(k)$: <br> - **Area**: Total pixels in region. <br> - **Bounding Box**: $((x, y, w, h))$ around the region. <br> - **Centroid**: $\left(\frac{\sum x}{\text{Area}}, \frac{\sum y}{\text{Area}}\right)$. | - Provides detailed region stats.<br>- Centroid is useful for feature extraction. | - Computationally intensive for large images.<br>- Sensitive to image noise. |
| `cv2.findContours(binaryImage, mode, method)` <br> Extracts contours of connected regions in a binary image. | Detects contours as outlines of connected components. | Uses boundary-following algorithms to trace connected regions. | - Provides explicit region boundaries.<br>- Supports hierarchical relationships. | - Only provides boundaries, not full region details.<br>- Requires preprocessing for accurate results. |

---

## Table 18: Statistical and Normalization Methods

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.meanStdDev(img, mask=None)` <br> Computes the mean and standard deviation of **img**, optionally using a **mask** to focus on specific regions. | Calculates basic statistics for image intensities. | - **Mean**: $\mu = \frac{1}{N} \sum_{i=1}^{N} I(x_i)$ <br> - **Standard Deviation**: $\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (I(x_i) - \mu)^2}$ <br> *where:* <br> $(N)$: Total pixels (or masked region) <br> $(I(x_i))$: Pixel intensity | - Simple and fast.<br>- Useful for image intensity analysis. | - Limited to basic statistics.<br>- No histogram information. |
| `cv2.normalize(img, None, alpha, beta, norm_type=cv2.NORM_MINMAX)` <br> Normalizes pixel values of **img** to a specified range [**alpha**, **beta**] using **norm_type**. | Scales image intensities to the desired range. | For `cv2.NORM_MINMAX`: $I_{\text{norm}}(x, y) = \frac{I(x, y) - I_{\text{min}}}{I_{\text{max}} - I_{\text{min}}} \cdot (\beta - \alpha) + \alpha$ <br> *where:* <br> $(I_{\text{min}}, I_{\text{max}})$: Minimum and maximum pixel intensities | - Normalizes data for better numerical stability.<br>- Flexible with multiple norm types. | - Requires parameter tuning for effective scaling.<br>- Sensitive to outliers for `NORM_MINMAX`. |
| `cv2.calcHist(images, channels, mask, histSize, ranges)` <br> Computes the histogram for a specified channel in **images** with optional **mask**. | Measures pixel intensity distribution across a specified range. | $H(i) = \sum_{x, y} \delta(I(x, y) - i)$ <br> *where:* <br> $(H(i))$: Frequency of intensity $(i)$ <br> $(\delta)$: Indicator function (1 if true, else 0) | - Provides detailed intensity distribution.<br>- Useful for contrast and brightness adjustments. | - Memory-intensive for large histograms.<br>- Requires preprocessing for multi-channel images. |
| `cv2.equalizeHist(img)` <br> Equalizes the histogram of a grayscale **img** for contrast enhancement. | Spreads pixel intensities across the available range. | $T(v) = \text{round} \left( \frac{\text{cdf}(v) - \text{cdf}_{\text{min}}}{(M \times N) - \text{cdf}_{\text{min}}} \cdot (L - 1) \right)$ <br> *where:* <br> $T(v)$: Transformed intensity value <br> ${cdf}(v)$: Cumulative histogram value <br> $(M, N)$: Image dimensions <br> $(L)$: Number of intensity levels | - Enhances low-contrast images.<br>- Automatic scaling for better visibility. | - Global operation.<br>- Can over-enhance some regions. |
| `cv2.mean(img, mask=None)` <br> Computes the mean value of **img**, optionally considering only the region defined by **mask**. | Calculates the average intensity over the image or masked area. | $\mu = \frac{1}{N} \sum_{i=1}^{N} I(x_i)$ <br> *where:* <br> $(N)$: Number of pixels in the region <br> $I(x_i)$: Intensity at pixel $i$ | - Simple calculation.<br>- Useful for average intensity analysis. | - Limited to a single statistic.<br>- No variance information. |
| `cv2.subtract(img1, img2)` <br> Subtracts pixel values of **img2** from **img1**, clamping at 0. | Calculates the intensity difference between two images. | $I_{\text{out}}(x, y) = \max(I_1(x, y) - I_2(x, y), 0)$ <br> *where:* <br> $I_1, I_2$: Pixel intensities in images 1 and 2 | - Useful for background subtraction.<br>- Highlights differences between images. | - Clipped at 0 for negative values.<br>- Requires same size and type for inputs. |

---

## Table 19: Feature Detection and Object Detection

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.FastFeatureDetector_create()` <br> Detects corners using the Features from Accelerated Segment Test (FAST) algorithm. | Identifies corners by comparing pixel intensities around a circle. | A pixel $(p)$ is a corner if there are \(n\) contiguous pixels on a circle brighter or darker than $(p)$: $\|I(p) - I(c)\| > t$ <br> *where:* <br> $(c)$: Neighboring pixels on the circle <br> $(t)$: Intensity threshold. | - Extremely fast.<br>- Suitable for real-time applications. | - No orientation or scale information.<br>- Sensitive to noise. |
| `cv2.CascadeClassifier.detectMultiScale(img, scaleFactor, minNeighbors, minSize)` <br> Detects objects (e.g., faces) in **img** using a cascade classifier. **scaleFactor** adjusts the size of the image at each scale, and **minNeighbors** defines required neighbor rectangles. | Performs multi-scale object detection using Haar-like or LBP features. | Sliding window detection with resizing and feature matching. | - Robust for pre-trained object types (e.g., faces).<br>- Handles multi-scale objects. | - Limited to predefined objects.<br>- Slow for large images. |
| `cv2.ORB_create()` <br> Detects and describes keypoints using the Oriented FAST and Rotated BRIEF (ORB) method. | Efficient and fast keypoint detection and descriptor extraction. | Combines FAST for corner detection and BRIEF for descriptors. <br> Orientation is determined using intensity-weighted centroid. | - Fast and efficient.<br>- Open-source alternative to SIFT. | - Not invariant to significant scale changes.<br>- May fail with high-frequency textures. |
| `cv2.SIFT_create()` <br> Detects keypoints and computes descriptors using the Scale-Invariant Feature Transform (SIFT) algorithm. | Identifies scale and rotation-invariant keypoints and their descriptors. | Keypoint detection uses difference-of-Gaussians: $D(x, y, \sigma) = L(x, y, k\sigma) - L(x, y, \sigma)$ <br> Descriptor: Local gradient histograms. | - Scale and rotation-invariant.<br>- Robust to changes in lighting. | - Computationally intensive.<br>- Requires careful threshold tuning. |
| `cv2.AKAZE_create()` <br> Detects and describes keypoints using the Accelerated-KAZE algorithm. | Extracts keypoints and descriptors efficiently using nonlinear scale spaces. | Nonlinear diffusion for scale space creation: $L(x, y, \sigma) = G(x, y, \sigma) \ast I(x, y)$ | - Scale and rotation-invariant.<br>- Faster than KAZE. | - Limited descriptor size.<br>- Less robust for very large-scale changes. |
| `cv2.BriefDescriptorExtractor_create()` <br> Creates a BRIEF descriptor extractor instance. | Extracts compact binary descriptors for given keypoints. | Encodes intensity comparisons into a binary string: $\text{Descriptor} = \{I(p_i) - I(p_j)\}_{i,j}$ | - Compact and efficient.<br>- Suitable for real-time matching. | - Not scale or rotation-invariant.<br>- Requires detected keypoints. |
| `compute(img, keypoints)` <br> Computes descriptors for the detected **keypoints** in **img**. | Extracts feature descriptors for keypoints. | Depends on the selected descriptor (e.g., SIFT, BRIEF). | - Customizable descriptor extraction.<br>- Works with various detectors. | - Requires detected keypoints.<br>- Dependent on descriptor type. |
| `cv2.BFMatcher.create(normType, crossCheck)` <br> Creates a Brute-Force matcher for descriptors. | Matches feature descriptors using a specified distance metric (e.g., Hamming or L2). | Matches descriptors $d_1$ and $d_2$ by minimizing: $\|d_1 - d_2\|$ | - Simple and easy to implement.<br>- Works with binary and floating-point descriptors. | - Computationally expensive for large datasets.<br>- Limited scalability. |
| `match(descriptors1, descriptors2)` <br> Matches descriptors from two feature sets. | Finds the best matching descriptor pairs. | Matches based on the selected norm type: $d_{match} = \min\{\|d_1 - d_2\|\}$ | - Straightforward matching.<br>- Flexible with descriptor types. | - Requires well-tuned thresholding.<br>- May result in false matches. |

---

## Table 20: Calibration and Stereo Functions

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.findChessboardCorners(image, patternSize)` <br> Finds the chessboard corners in **image** based on the **patternSize** grid. | Detects 2D coordinates of calibration target points. | Uses pattern matching and corner refinement to locate key points. | - Essential for camera calibration.<br>- Works well with high-contrast images. | - Sensitive to image quality and lighting.<br>- Requires predefined pattern. |
| `cv2.cornerSubPix(image, corners, winSize, zeroZone, criteria)` <br> Refines corner coordinates to sub-pixel accuracy. | Improves the accuracy of detected corners. | Iteratively optimizes corner locations using termination **criteria**: $(\text{Max Iterations}, \text{Epsilon})$. | - Enhances precision of detected points.<br>- Effective for calibration. | - Computationally intensive.<br>- Requires initial corner estimates. |
| `cv2.calibrateCamera(objectPoints, imagePoints, imageSize, cameraMatrix, distCoeffs)` <br> Computes intrinsic and extrinsic parameters for a camera. | Calculates the camera matrix **K**, distortion coefficients **D**, rotation matrices **R**, and translation vectors **T**. | Solves: $P = K[R\|T] X$ <br> - Intrinsic: $K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$ <br> - Distortion modeled with **D**. | - Provides comprehensive camera parameters.<br>- Useful for 3D reconstruction. | - Requires multiple images.<br>- Sensitive to corner detection accuracy. |
| `cv2.drawChessboardCorners(image, patternSize, corners, found)` <br> Draws the detected corners on **image** for visualization. | Overlays detected calibration points on the image. | Visually represents corner locations from `findChessboardCorners`. | - Assists in debugging calibration accuracy.<br>- Easy to use. | - Limited to visualization.<br>- No additional data analysis. |
| `cv2.undistort(image, cameraMatrix, distCoeffs)` <br> Corrects image distortions based on camera parameters. | Removes distortion using intrinsic parameters. | Transforms pixel coordinates $(u, v)$ to undistorted ones: $[x, y] = f^{-1}(K, D, u, v)$. | - Improves image quality.<br>- Prepares images for accurate measurements. | - Requires calibrated parameters.<br>- May introduce cropping artifacts. |
| `cv2.StereoSGBM_create(minDisparity, numDisparities, blockSize)` <br> Creates a Semi-Global Block Matching stereo matcher. | Estimates dense disparity maps for stereo images. | Aggregates matching cost using a semi-global optimization method. | - High-quality disparity maps.<br>- Robust to occlusions. | - Computationally expensive.<br>- Sensitive to parameter tuning. |
| `cv2.StereoBM_create(numDisparities, blockSize)` <br> Creates a simpler block matching stereo matcher. | Computes disparity maps using block matching. | Matches pixel blocks between left and right images. | - Fast and easy to use.<br>- Low resource requirements. | - Produces noisier disparity maps.<br>- Less robust in textureless regions. |
| `cv2.compute(leftImage, rightImage)` <br> Computes the disparity map from stereo images. | Outputs disparity values for each pixel in the left image. | Solves stereo correspondence problem. | - Enables depth map estimation.<br>- Useful for 3D reconstruction. | - Requires rectified images.<br>- Dependent on stereo matching algorithm. |

---

## Table 21: Motion and Tracking Functions

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance)` <br> Identifies key points (corners) in an image. | Uses Shi-Tomasi corner detection to find significant points for tracking. | Maximizes the eigenvalues of the gradient matrix: $\lambda = \min(\lambda_1, \lambda_2)$. | - Fast and reliable.<br>- Finds strong tracking features. | - Limited to corners.<br>- Can miss features in textureless areas. |
| `cv2.calcOpticalFlowPyrLK(prevImage, nextImage, prevPoints, None)` <br> Tracks points across frames using optical flow. | Estimates motion between two frames for given points. | Solves for $\mathbf{d}$ in $I(x+\mathbf{d}, t+1) \approx I(x, t)$ using image pyramids. | - Accurate for small motions.<br>- Efficient with pyramidal implementation. | - Fails for large displacements.<br>- Sensitive to occlusions. |
| `cv2.TrackerCSRT_create()` <br> Instantiates the CSRT tracker for object tracking. | Initializes a robust tracker using correlation filters. | Combines kernelized correlation filters with spatial constraints for improved accuracy. | - Accurate and robust.<br>- Handles scale and rotation changes. | - Computationally expensive.<br>- Requires manual initialization. |
| `init(frame, bbox)` <br> Initializes the tracker with the first frame and bounding box. | Sets the region of interest (ROI) for tracking. | Defines the bounding box: $[x, y, \text{width}, \text{height}]$. | - Provides precise starting location.<br>- Necessary for accurate tracking. | - Requires user input.<br>- Not adaptive to initialization errors. |
| `update(frame)` <br> Updates the tracker with a new frame and computes the new bounding box. | Tracks the object across frames and returns the new bounding box. | Uses the CSRT algorithm to estimate the new location of the object in the frame. | - Effective for continuous tracking.<br>- Handles challenging scenarios like partial occlusions. | - May drift if the object changes drastically.<br>- Limited to one object per tracker instance. |

---

Here’s a new table for **Pixel Count and Mask Analysis**, including `cv2.countNonZero` and similar functions:

---

## Table 22: Pixel Count and Mask Analysis

| **Function & Arguments** | **Description** | **Formula & Details** | **Advantages** | **Disadvantages** |
|---------------------------|-----------------|------------------------|----------------|--------------------|
| `cv2.countNonZero(src)` <br> Counts the number of non-zero pixels in the input image **src**. | Calculates the number of non-zero pixels in a binary or grayscale image. | $N_{\text{non-zero}} = \sum_{x, y} \delta(I(x, y))$ <br> *where:* <br> $\delta(I(x, y)) = \begin{cases} 1 & \text{if } I(x, y) \neq 0 \\ 0 & \text{otherwise} \end{cases}$ | - Fast and efficient.<br>- Useful for analyzing masks or binary images. | - Limited to counting.<br>- Does not provide pixel locations. |
| `np.sum(src > 0)` (NumPy) <br> Counts the number of pixels greater than 0 in **src** using NumPy. | Alternative to `cv2.countNonZero` for more flexibility. | Similar to $N_{\text{non-zero}}$ using NumPy’s array logic. | - Highly flexible.<br>- Works with multi-dimensional arrays. | - Slower for large images compared to OpenCV. |
| `cv2.findNonZero(src)` <br> Finds the coordinates of all non-zero pixels in **src**. | Returns a list of the coordinates of non-zero pixels. | $P = \{(x, y) \mid I(x, y) \neq 0\}$ | - Provides exact pixel locations.<br>- Useful for region analysis. | - Memory-intensive for dense masks.<br>- Slower for large-scale data. |
| `cv2.compare(src1, src2, cmpop)` <br> Performs pixel-wise comparison between **src1** and **src2** using an operator **cmpop** (e.g., `cv2.CMP_GT`, `cv2.CMP_EQ`). | Generates a binary mask based on the comparison. | Compares pixel values: $R(x, y) = \begin{cases} 255 & \text{if comparison is true} \\ 0 & \text{otherwise} \end{cases}$ | - Flexible with multiple comparison operators.<br>- Useful for mask creation. | - Limited to pixel-wise comparisons.<br>- Requires same dimensions for inputs. |
