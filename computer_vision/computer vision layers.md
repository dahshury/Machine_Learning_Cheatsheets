### PyTorch Computer Vision Layers

| **Layer**                   | **Purpose**                                            | **Mathematical Formula**                                     | **Key Points**                                                 |
|-----------------------------|--------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------|
| **Conv2d**                  | Applies a 2D convolution over an input image.           | \[ \text{Conv}(x) = (x * W + b) \]                          | Kernel size, padding, and stride are key tunable parameters. Efficient feature extraction in images. |
| **BatchNorm2d**             | Normalizes activations over a mini-batch.               | \[ \hat{x} = \frac{x - \mu}{\sigma} \gamma + \beta \]        | Helps stabilize and speed up training. Reduces internal covariate shift. |
| **ReLU**                    | Applies the rectified linear activation function.       | \[ \text{ReLU}(x) = \max(0, x) \]                           | Introduces non-linearity; simple and widely used in CNNs. |
| **MaxPool2d**               | Downsamples the input by taking the maximum value over a window. | \[ \text{MaxPool}(x) = \max(x_i) \text{ for } i \in \text{window} \] | Reduces spatial dimensions, keeping important features. Helps control overfitting. |
| **AvgPool2d**               | Downsamples the input by averaging the values in a window. | \[ \text{AvgPool}(x) = \frac{1}{n} \sum_{i=1}^n x_i \]        | Smoother downsampling; preserves more context than MaxPooling. |
| **AdaptiveAvgPool2d**       | Downsamples input to a fixed size.                      | -                                                            | Output size is independent of input size, commonly used before fully connected layers. |
| **Dropout2d**               | Randomly zeros out entire channels in 2D feature maps.  | -                                                            | Regularization technique to prevent overfitting in CNNs. |
| **Linear**                  | Fully connected layer, applies linear transformation.   | \[ \text{Linear}(x) = xW + b \]                              | Used for final classification after convolutional layers. |
| **ConvTranspose2d**         | Applies a transposed convolution (used for upsampling). | \[ \text{ConvTranspose}(x) = (x * W^T + b) \]                | Used in decoder networks (e.g., for image generation or segmentation). |
| **LeakyReLU**               | Variant of ReLU that allows small gradients when \( x < 0 \). | \[ \text{LeakyReLU}(x) = \begin{cases} x, & \text{if } x \geq 0 \\ \alpha x, & \text{if } x < 0 \end{cases} \] | Avoids dying ReLU problem by allowing small negative values. |
| **Sigmoid**                 | Squashes input to a range \([0, 1]\).                  | \[ \sigma(x) = \frac{1}{1 + e^{-x}} \]                       | Often used in binary classification tasks. |
| **Tanh**                    | Squashes input to a range \([-1, 1]\).                 | \[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]            | Useful in tasks requiring negative activations. |
| **Softmax**                 | Converts logits into probabilities over multiple classes. | \[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} \]    | Used in multi-class classification problems. |
| **Flatten**                 | Flattens a multi-dimensional tensor to a 1D tensor.     | -                                                            | Prepares feature maps for fully connected layers. |
| **Upsample**                | Upsamples input tensor to a higher resolution.         | -                                                            | Simple, fast upsampling often used in segmentation. |
| **ELU (Exponential Linear Unit)** | Smooth activation that allows negative values.     | \[ \text{ELU}(x) = \begin{cases} x, & \text{if } x \geq 0 \\ \alpha (e^x - 1), & \text{if } x < 0 \end{cases} \] | Faster convergence compared to ReLU; allows negative outputs. |
| **SELU**                    | Scaled Exponential Linear Unit. Normalizes activations like BatchNorm. | \[ \text{SELU}(x) = \lambda \begin{cases} x, & \text{if } x \geq 0 \\ \alpha (e^x - 1), & \text{if } x < 0 \end{cases} \] | Self-normalizing, ensures mean and variance are maintained. |
| **PixelShuffle**            | Rearranges elements to upscale spatial dimensions.     | -                                                            | Used for upsampling, especially in super-resolution tasks. |
