### PyTorch Computer Vision Layers

| **Layer**                   | **Purpose**                                            | **Mathematical Formula**                                     | **Key Points**                                                 |
|-----------------------------|--------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------|
| **Conv2d**                  | Applies a 2D convolution over an input image.           | ${Conv}(x) = (x * W + b)$                                    | Kernel size, padding, and stride are key tunable parameters. Efficient feature extraction in images. |
| **BatchNorm2d**             | Normalizes activations over a mini-batch.               | $\hat{x} = \frac{x - \mu}{\sigma} \gamma + \beta$            | Helps stabilize and speed up training. Reduces internal covariate shift. |
| **ReLU**                    | Applies the rectified linear activation function.       | ${ReLU}(x) = \max(0, x)$                                     | Introduces non-linearity; simple and widely used in CNNs. |
| **MaxPool2d**               | Downsamples the input by taking the maximum value over a window. | ${MaxPool}(x) = \max(x_i) \text{ for } i \in \text{window}$   | Reduces spatial dimensions, keeping important features. Helps control overfitting. |
| **AvgPool2d**               | Downsamples the input by averaging the values in a window. | ${AvgPool}(x) = \frac{1}{n} \sum_{i=1}^n x_i$                | Smoother downsampling; preserves more context than MaxPooling. |
| **Dropout2d**               | Randomly zeros out entire channels in 2D feature maps.  | -                                                            | Regularization technique to prevent overfitting in CNNs. |
| **Linear**                  | Fully connected layer, applies linear transformation.   | ${Linear}(x) = xW + b$                                       | Used for final classification after convolutional layers. |
| **ConvTranspose2d**         | Applies a transposed convolution (used for upsampling). | ${ConvTranspose}(x) = (x * W^T + b)$                         | Used in decoder networks (e.g., for image generation or segmentation). |
| **LeakyReLU**               | Variant of ReLU that allows small gradients when $x < 0$. | ${LeakyReLU}(x) = \begin{cases} x, & \text{if } x \geq 0 \\ \alpha x, & \text{if } x < 0 \end{cases}$ | Avoids dying ReLU problem by allowing small negative values. |
| **Sigmoid**                 | Squashes input to a range $[0, 1]$.                     | $\sigma(x) = \frac{1}{1 + e^{-x}}$                           | Often used in binary classification tasks. |
| **Tanh**                    | Squashes input to a range $[-1, 1]$.                    | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$               | Useful in tasks requiring negative activations. |
| **Softmax**                 | Converts logits into probabilities over multiple classes. | ${Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$             | Used in multi-class classification problems. |
| **Flatten**                 | Flattens a multi-dimensional tensor to a 1D tensor.     | -                                                            | Prepares feature maps for fully connected layers. |
| **Upsample**                | Upsamples input tensor to a higher resolution.          | -                                                            | Simple, fast upsampling often used in segmentation. |
| **ELU (Exponential Linear Unit)** | Smooth activation that allows negative values.     | $$\text{ELU}(x) = \begin{cases} x, & \quad \text{for } x \geq 0 \\alpha (e^x - 1), & \quad \text{for } x < 0\end{cases}$$ | Faster convergence compared to ReLU; allows negative outputs. |
| **SELU**                    | Scaled Exponential Linear Unit. Normalizes activations like BatchNorm. | $\text{SELU}(x) = \lambda \begin{cases} x, & \text{if } x \geq 0 \\ \alpha (e^x - 1), & \text{if } x < 0 \end{cases}$ | Self-normalizing, ensures mean and variance are maintained. |
| **PixelShuffle**            | Rearranges elements to upscale spatial dimensions.      | -                                                            | Used for upsampling, especially in super-resolution tasks. |
