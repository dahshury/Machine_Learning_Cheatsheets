| Loss Function          | Purpose | Formula | Key Points |
|------------------------|---------|---------|------------|
| **Mean Bias Error**    | Captures average bias in prediction, but is rarely used for training | **$\text{MBE} = \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $N$: Number of samples | - Provides average bias<br>- Does not penalize large errors |
| **Mean Absolute Error** | Measures absolute average bias in prediction, also known as L1 Loss | **$\text{MAE} = \frac{1}{N} \sum_{i=1}^N  \|y_i - f(x_i)\|$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $N$: Number of samples | - Robust to outliers<br>- Provides a direct measure of average error |
| **Mean Squared Error**  | Average squared distance between actual and predicted values, also known as L2 Loss | **$\text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $N$: Number of samples | - Penalizes larger errors more than smaller ones<br>- Commonly used in regression |
| **Root Mean Squared Error** | Square root of MSE; provides error in the same units as the dependent variable | **$\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2}$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $N$: Number of samples | - Easier to interpret as it has the same units as the target variable<br>- Sensitive to large errors |
| **Huber Loss**         | Combines MSE and MAE; a parametric loss function | **$\text{Huber Loss} = \begin{cases} \frac{1}{2} (y_i - f(x_i))^2 & \text{if }  \|y_i - f(x_i)\|  \leq \delta \\ \delta \cdot (\|y_i - f(x_i)\| - \frac{1}{2} \delta) & \text{otherwise} \end{cases}$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $\delta$: Threshold parameter | - Smooth transition between MAE and MSE<br>- Less sensitive to outliers compared to MSE |
| **Log-Cosh Loss**     | Similar to Huber Loss but non-parametric; computationally expensive | **$\text{Log-Cosh Loss} = \frac{1}{N} \sum_{i=1}^N \log(\cosh(f(x_i) - y_i))$** <br><br> *where:* <br> $y_i$: True value <br> $f(x_i)$: Predicted value <br> $N$: Number of samples | - Smooth and differentiable<br>- Computationally intensive |
| **Binary Cross-Entropy (BCE) Loss** | Loss function for binary classification tasks | **$\text{BCE Loss} = -\frac{1}{N} \sum_{i=1}^N [y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i))]$** <br><br> *where:* <br> $y_i$: True label (0 or 1) <br> $p(x_i)$: Predicted probability <br> $N$: Number of samples | - Suitable for binary classification<br>- Sensitive to class imbalance |
| **Categorical Cross-Entropy Loss** | Extension of BCE loss to multi-class classification | **$\text{CE Loss} = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M y_{ij} \log(f(x_{ij}))$** <br><br> *where:* <br> $y_{ij}$: True probability for class $j$ <br> $f(x_{ij})$: Predicted probability for class $j$ <br> $N$: Number of samples <br> $M$: Number of classes | - Suitable for multi-class classification<br>- Assumes mutually exclusive classes |
| **Kullback-Leibler Divergence** | Minimizes the divergence between predicted and true probability distributions | **$\text{KL Divergence} = \sum_{i=1}^N y_i \log \left(\frac{y_i}{f(x_i)}\right)$** <br><br> *where:* <br> $y_i$: True probability distribution <br> $f(x_i)$: Predicted probability distribution <br> $N$: Number of samples | - Measures divergence between distributions<br>- Not symmetric |
