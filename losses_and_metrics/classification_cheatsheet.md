# Classification Metrics and Losses Cheat Sheet

## Table 1: Classification Metrics

| Metric              | Purpose | Formula | Key Points |
|---------------------|---------|---------|------------|
| **Accuracy**        | Measures the overall correctness of the model | ${\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}}$ <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{TN}$: True Negatives <br> $\text{FP}$: False Positives <br> $\text{FN}$: False Negatives | - Simple and intuitive<br>- Can be misleading for imbalanced datasets |
| **Precision**       | Measures the accuracy of positive predictions | ${\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}}$ <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{FP}$: False Positives | - Important when false positives are costly<br>- Does not account for false negatives |
| **Recall**          | Measures the ability to find all positive instances | ${\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}}$ <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{FN}$: False Negatives | - Important when false negatives are costly<br>- Does not account for false positives |
| **F1 Score**        | Harmonic mean of precision and recall | ${\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}}$ <br><br> *where:* <br> $\text{Precision}$: Precision <br> $\text{Recall}$: Recall | - Balances precision and recall<br>- Useful for imbalanced datasets |
| **Specificity**     | Measures the ability to identify negative instances | ${\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}}$ <br><br> *where:* <br> $\text{TN}$: True Negatives <br> $\text{FP}$: False Positives | - Complement to recall for the negative class<br>- Less relevant in highly imbalanced datasets |
| **ROC AUC**         | Measures model's ability to distinguish between classes | ${\text{AUC} = \int \text{TPR}(T) \, \text{FPR}'(T) \, \text{d}T}$ <br><br> *where:* <br> $\text{TPR}$: True Positive Rate <br> $\text{FPR}$: False Positive Rate | - Measures overall discrimination capability<br>- Insensitive to class imbalance |
| **Precision-Recall AUC** | Measures the trade-off between precision and recall | ${\text{PR AUC} = \int \text{Precision}(R) \, \text{d}R}$ <br><br> *where:* <br> $\text{Precision}(R)$: Precision at recall level $R$ | - Better for imbalanced datasets than ROC AUC<br>- Focuses on the positive class |
| **Cohen's Kappa**   | Measures agreement between two raters | ${\kappa = \frac{\text{po} - \text{pe}}{1 - \text{pe}}}$ <br><br> *where:* <br> $\text{po}$: Observed agreement <br> $\text{pe}$: Expected agreement by chance | - Accounts for chance agreement<br>- Useful for multi-class and imbalanced problems |
| **Matthews Correlation Coefficient** | Measures the quality of binary classifications | ${\text{MCC} = \frac{\text{TP} \cdot \text{TN} - \text{FP} \cdot \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FN})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}}$ <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{TN}$: True Negatives <br> $\text{FP}$: False Positives <br> $\text{FN}$: False Negatives | - Considered a balanced measure<br>- Works well even with class imbalance |
| **Miss Rate**       | Measures the proportion of actual positives that were missed | ${\text{Miss Rate} = \frac{\text{FN}}{\text{FN} + \text{TP}}}$ <br><br> *where:* <br> $\text{FN}$: False Negatives <br> $\text{TP}$: True Positives | - Useful for evaluating how often objects are missed<br>- Complementary to Recall |
| **False Alarm Rate**| Measures the rate of false positives | ${\text{False Alarm Rate} = \frac{\text{FP}}{\text{FP} + \text{TN}}}$ <br><br> *where:* <br> $\text{FP}$: False Positives <br> $\text{TN}$: True Negatives | - Important for understanding false positives<br>- Less relevant in some object detection contexts |

---

## Table 2: Losses for Classification Tasks

| Loss Function             | Purpose | Formula | Key Points |
|---------------------------|---------|---------|------------|
| **Binary Cross-Entropy Loss** | Measures the performance of a binary classification model | ${\text{BCE Loss} = -[y \log(p) + (1 - y) \log(1 - p)]}$ <br><br> *where:* <br> $y$: True label (0 or 1) <br> $p$: Predicted probability for the positive class | - Suitable for binary classification<br>- Sensitive to class imbalance |
| **Categorical Cross-Entropy Loss** | Measures the performance of a multi-class classification model | ${\text{CCE Loss} = -\sum_{i} y_i \log(p_i)}$ <br><br> *where:* <br> $y_i$: True probability for class $i$ <br> $p_i$: Predicted probability for class $i$ | - Suitable for multi-class classification<br>- Assumes mutually exclusive classes |
| **Focal Loss**            | Addresses class imbalance by down-weighting easy examples | ${\text{Focal Loss} = -\alpha (1 - p_t)^\gamma \log(p_t)}$ <br><br> *where:* <br> $\alpha$: Balancing factor <br> $p_t$: Predicted probability for the target class <br> $\gamma$: Focusing parameter | - Focuses on hard examples<br>- Reduces the impact of easy examples |
| **Hinge Loss**            | Used in support vector machines for "maximum-margin" classification | ${\text{Hinge Loss} = \max(0, 1 - y \cdot f(x))}$ <br><br> *where:* <br> $y$: True label (-1 or 1) <br> $f(x)$: Model's prediction | - Promotes large-margin classification<br>- Not probabilistic |
| **Squared Hinge Loss**    | A smoother version of hinge loss | ${\text{Squared Hinge Loss} = \max(0, 1 - y \cdot f(x))^2}$ <br><br> *where:* <br> $y$: True label (-1 or 1) <br> $f(x)$: Model's prediction | - Differentiable everywhere<br>- Penalizes large margin violations more than hinge loss |
| **Kullback-Leibler Divergence** | Measures the difference between two probability distributions | ${\text{KL}(P \parallel Q) = \sum_{i} P(i) \log \left(\frac{P(i)}{Q(i)}\right)}$ <br><br> *where:* <br> $P(i)$: True distribution <br> $Q(i)$: Predicted distribution | - Measures divergence between distributions<br>- Not symmetric |
| **Mean Squared Error (for probabilities)** | Measures the average squared difference between predicted and true probabilities | ${\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$ <br><br> *where:* <br> $y_i$: True probability <br> $p_i$: Predicted probability <br> $n$: Number of samples | - Common in regression<br>- Not commonly used for classification |
| **Exponential Loss**      | Exponentially penalizes misclassifications | ${\text{Exponential Loss} = \exp(-y \cdot f(x))}$ <br><br> *where:* <br> $y$: True label (-1 or 1) <br> $f(x)$: Model's prediction | - Used in boosting algorithms (e.g., AdaBoost)<br>- Sensitive to outliers and noise |
| **Contrastive Loss**      | Learn to group similar instances and separate dissimilar ones | ${\text{Contrastive Loss} = y \cdot d^2 + (1 - y) \cdot \max(\text{margin} - d, 0)^2}$ <br><br> *where:* <br> $y$: 1 for similar pairs, 0 for dissimilar pairs <br> $d$: Distance between two points <br> $\text{margin}$: Hyperparameter | - Effective for learning embeddings<br>- Requires careful margin tuning |
| **Triplet Loss**          | Learn to rank similar instances closer than dissimilar ones | ${\text{Triplet Loss} = \max(d(a, p) - d(a, n) + \text{margin}, 0)}$ <br><br> *where:* <br> $a$: Anchor <br> $p$: Positive example <br> $n$: Negative example <br> $d$: Distance function <br> $\text{margin}$: Hyperparameter | - Effective for learning embeddings<br>- Requires triplet sampling and margin tuning |
| **L1 Loss**               | Measures the average absolute difference between predicted and true values | ${\text{L1 Loss} = \sum_{i=1}^n \|y_i - \hat{y}_i\|}$ <br><br> *where:* <br> $y_i$: True value <br> $\hat{y}_i$: Predicted value <br> $n$: Number of samples | - Robust to outliers<br>- Less sensitive than MSE |
