### Object Detection Metrics

| **Metric**       | **Purpose**                                                | **Mathematical Formula**                                      | **Key Points**                                  |
|------------------|------------------------------------------------------------|---------------------------------------------------------------|------------------------------------------------|
| **Precision**    | Accuracy of positive predictions                          | **$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$** <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{FP}$: False Positives                  | Measures the fraction of true positives among predicted positives. |
| **Recall**       | Ability to find all positive instances                     | **$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$** <br><br> *where:* <br> $\text{TP}$: True Positives <br> $\text{FN}$: False Negatives                     | Indicates the fraction of actual positives that are correctly predicted. |
| **F1 Score**     | Harmonic mean of Precision and Recall                      | **$\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$** <br><br> *where:* <br> $\text{Precision}$: Ratio of true positives to the sum of true positives and false positives <br> $\text{Recall}$: Ratio of true positives to the sum of true positives and false negatives | Balances precision and recall into a single metric. |
| **IoU**          | Measures overlap between predicted and ground truth boxes  | $ \text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}} $ | Basic metric for evaluating bounding box accuracy. |
| **GIoU**         | Generalized IoU, accounts for overlap and bounding box sizes | **$\text{GIoU} = \text{IoU} - \frac{\text{Area of Enclosing Box} - \text{Area of Union}}{\text{Area of Enclosing Box}}$** <br><br> *where:* <br> $\text{IoU}$: Intersection over Union between the predicted box and the ground truth box <br> $\text{Area of Union}$: Combined area of the predicted box and the ground truth box <br> $\text{Area of Enclosing Box}$: Area of the smallest box that can enclose both the predicted box and the ground truth box | Addresses limitations of IoU by considering the enclosing box. |
| **DIoU**         | Distance IoU, includes distance between centers of boxes   | **$\text{DIoU} = \text{IoU} - \frac{d^2(b, b^g)}{c^2}$** <br><br> *where:* <br> $\text{IoU}$: Intersection over Union between the predicted box and the ground truth box <br> $d(b, b^g)$: Euclidean distance between the center points of the predicted box $b$ and the ground truth box $b^g$ <br> $c$: Diagonal length of the smallest enclosing box covering both the predicted box and the ground truth box             | Improves upon GIoU by penalizing distance between box centers. |
| **Complete IoU (CIoU)** | Measures the overlap between two bounding boxes while accounting for distance, aspect ratio, and IoU. | **$\text{CIoU} = \text{IoU} - \left(\frac{\rho^2(\mathbf{b}, \mathbf{b}^g)}{c^2} + \alpha \cdot v\right)$** <br><br> **Where:** <br> $\text{IoU}$: Intersection over Union <br> $\rho(\mathbf{b}, \mathbf{b}^g)$: Euclidean distance between the centers of the predicted box $\mathbf{b}$ and ground truth box $\mathbf{b}^g$ <br> $c$: Diagonal length of the smallest enclosing box covering both $\mathbf{b}$ and $\mathbf{b}^g$ <br> $\alpha$: Trade-off parameter defined as $\alpha = \frac{v}{(1 - \text{IoU}) + v}$ <br> $v$: Aspect ratio consistency term, defined as $v = \frac{4}{\pi^2} \left(\arctan\left(\frac{w^g}{h^g}\right) - \arctan\left(\frac{w}{h}\right)\right)^2$ <br> $w$, $h$: Width and height of the predicted bounding box <br> $w^g$, $h^g$: Width and height of the ground truth bounding box | - **Pros:** Incorporates aspect ratio and distance, leading to more robust performance in object detection. <br> - **Cons:** Slightly more complex computation compared to IoU and GIoU. |
| **Top-K Accuracy** | Measures if the correct label is among the top-K predictions | **$\text{Top-k Accuracy} = \frac{\text{Number of Correct Predictions in Top k}}{\text{Total Number of Predictions}}$** <br><br> *where:* <br> $\text{Number of Correct Predictions in Top k}$: Number of times the true label is among the top $k$ predicted probabilities <br> $\text{Total Number of Predictions}$: Total number of samples evaluated | Useful in scenarios where top-K predictions are considered. |
| **Miss Rate**    | Fraction of missed detections                              | **$\text{Miss Rate} = \frac{\text{FN}}{\text{FN} + \text{TP}}$** <br><br> *where:* <br> $\text{FN}$: False Negatives <br> $\text{TP}$: True Positives                  | Evaluates the proportion of false negatives. |
| **False Alarm Rate** | Fraction of false positives                             | **$\text{False Alarm Rate} = \frac{\text{FP}}{\text{FP} + \text{TN}}$** <br><br> *where:* <br> $\text{FP}$: False Positives <br> $\text{TN}$: True Negatives           | Measures the rate of false positives among all negatives. |

### Object Detection Losses

| **Loss**               | **Purpose**                                           | **Mathematical Formula**                                         | **Key Points**                                      |
|------------------------|-------------------------------------------------------|------------------------------------------------------------------|----------------------------------------------------|
| **Focal Loss**     | Addresses the issue of class imbalance by down-weighting the loss for well-classified examples, focusing more on hard examples. | **$\text{Focal Loss} = - \alpha_t (1 - p_t)^\gamma \log(p_t)$** <br><br> **Where:** <br> $\alpha_t$: Balancing factor for class $t$ <br> $p_t$: Predicted probability for the true class $t$ <br> $\gamma$: Focusing parameter,  (typically 0.5 to 5), controls the down-weighting of easy examples <br> | - **Pros:** Effective for dealing with class imbalance, particularly in object detection tasks. helps models focus on hard-to-classify examples during training. <br> - **Cons:** Introduces additional hyperparameters ($\alpha$ and $\gamma$) that need to be tuned. |
| **BCE Loss**           | Binary Cross-Entropy loss for binary classification   | **$\text{BCE Loss} = -[y \log(p) + (1-y) \log(1-p)]$** <br><br> *where:* <br> $y$: True label (0 or 1) <br> $p$: Predicted probability for the positive class | Commonly used for binary classification problems. |
| **BCE with Logits**    | BCE with logits; combines sigmoid and BCE loss       | **$\text{BCE Loss with Logits} = \text{BCE}(\sigma(z), y)$** <br><br> *where:* <br> $z$: Logits, raw model output before applying a sigmoid function <br> $\sigma(z)$: Sigmoid function applied to logits, producing the probability $p$ <br> $y$: True label (0 or 1)            | Numerical stability improvement over BCE loss.  |
| **L1 Smooth Loss**     | L1 loss with smoothing to handle small errors          | **$\text{Smooth L1} = \begin{cases} 0.5 \times (y - \hat{y})^2 & \text{if }  \|y - \hat{y}\|  < \delta \\ \delta \times  \|y - \hat{y}\|  - 0.5 \times \delta^2 & \text{otherwise} \end{cases}$** | (type:box loss) Robust to outliers, balances between L1 and L2 loss. |
| **VFL (Varifocal Loss)** | Adaptively adjusts focal loss based on prediction quality | **$\text{VFL Loss} = \sum -\alpha_t (1 - p_t)^\gamma \log(p_t)$** <br><br> *where:* <br> $\alpha_t$: Adaptive weight based on IoU or another metric <br> $p_t$: Model's predicted probability for the target class $y$ <br> $\gamma$: Focusing parameter to down-weight easy examples | Adjusts loss weighting based on the prediction variance. |
| **L1 Loss**            | Measures absolute differences between predicted and true values                     | **$\text{L1 Loss} = \sum_{i=1}^n  \|y_i - \hat{y}_i\| $** <br><br> *where:* <br> $y_i$: True value <br> $\hat{y}_i$: Predicted value <br> $n$: Number of samples | Less sensitive to outliers compared to L2 loss. |
