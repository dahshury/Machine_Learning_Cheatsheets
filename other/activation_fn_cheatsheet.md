### Activation Functions

| Activation Function | Purpose | Formula | Key Points |
|---------------------|---------|---------|------------|
| **ReLU**            | Provides output as-is for positive inputs, zero for negative inputs, widely used | **$\text{ReLU}(x) = \max(0, x)$** <br><br> *where:* <br> $x$: Input value | - **Pros:** Simple, computationally efficient, reduces vanishing gradient <br> - **Cons:** Dying ReLU problem (units can become inactive) |
| **Leaky ReLU**      | Similar to ReLU but allows a small gradient when $x$ is negative | **$\text{Leaky ReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$** <br><br> *where:* <br> $x$: Input value <br> $\alpha$: Small positive constant | - **Pros:** Mitigates dying ReLU problem, maintains gradient for negative inputs <br> - **Cons:** Less interpretable than ReLU, $\alpha$ needs tuning |
| **PReLU**           | Extends Leaky ReLU with learnable parameter for negative inputs | **$\text{PReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$** <br><br> *where:* <br> $x$: Input value <br> $\alpha$: Learnable parameter | - **Pros:** Adaptive to different datasets, learns optimal $\alpha$ <br> - **Cons:** Increased complexity, requires additional training parameters |
| **ELU**             | Exponential Linear Unit, smoothens activation for negative values | **$\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (\exp(x) - 1) & \text{if } x \leq 0 \end{cases}$** <br><br> *where:* <br> $x$: Input value <br> $\alpha$: Positive constant | - **Pros:** Reduces vanishing gradient, output zero-centered <br> - **Cons:** Computationally expensive, $\alpha$ needs tuning |
| **Swish**           | Combines sigmoid and linear functions, smooth non-linearity | **$\text{Swish}(x) = x \cdot \sigma(x)$** <br><br> *where:* <br> $x$: Input value <br> $\sigma(x)$: Sigmoid function | - **Pros:** Smooth gradient, better performance on some tasks <br> - **Cons:** More computationally intensive, less interpretable |
| **GELU**            | Gaussian Error Linear Unit, provides smoother non-linearity | **$\text{GELU}(x) = x \cdot \Phi(x)$** <br><br> *where:* <br> $x$: Input value <br> $\Phi(x)$: Cumulative distribution function of the standard normal distribution | - **Pros:** Smooth and continuous, better performance in some deep networks <br> - **Cons:** Computationally intensive |
| **Softplus**        | Smooth approximation to ReLU, maps inputs to a range between 0 and âˆž | **$\text{Softplus}(x) = \log(1 + \exp(x))$** <br><br> *where:* <br> $x$: Input value | - **Pros:** Smooth gradient, avoids dead neurons <br> - **Cons:** Computationally expensive, can be similar to ReLU with large inputs |
| **Sigmoid**         | Maps inputs to a range between 0 and 1, useful for binary classification | **$\sigma(x) = \frac{1}{1 + \exp(-x)}$** <br><br> *where:* <br> $x$: Input value | - **Pros:** Smooth gradient, interpretable output <br> - **Cons:** Vanishing gradient problem, output not zero-centered |
| **Binary Step**     | Maps inputs to binary values (0 or 1), used in early neural networks | **$\text{Binary Step}(x) = \begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 \end{cases}$** <br><br> *where:* <br> $x$: Input value | - **Pros:** Simple, easy to implement <br> - **Cons:** Non-differentiable, not suitable for gradient-based optimization |
| **Linear**          | Outputs the input directly, used in the output layer for regression | **$\text{Linear}(x) = x$** <br><br> *where:* <br> $x$: Input value | - **Pros:** Direct output, suitable for regression tasks <br> - **Cons:** No non-linearity, not suitable for hidden layers |
