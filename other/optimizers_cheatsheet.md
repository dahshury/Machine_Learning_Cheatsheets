### Optimizers

| Optimizer                           | Purpose | Formula | Key Points |
|-------------------------------------|---------|---------|------------|
| **SGD**           | Simple optimizer using gradients to update parameters | **$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} \mathcal{L}(\theta_t)$** <br><br> *where:* <br> $\theta_t$: Parameters at iteration $t$ <br> $\eta$: Learning rate <br> $\nabla_{\theta} \mathcal{L}$: Gradient of the loss function | - **Pros:** Simple, easy to implement, requires minimal memory <br> - **Cons:** Can be slow to converge, sensitive to learning rate |
| **Momentum**                        | Improves SGD by adding momentum term to accelerate convergence | **$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} \mathcal{L}(\theta_t) + \mu \cdot v_t$** <br><br> *where:* <br> $v_t$: Velocity at iteration $t$ <br> $\mu$: Momentum factor | - **Pros:** Accelerates convergence, reduces oscillations <br> - **Cons:** Requires tuning of momentum parameter, can overshoot |
| **Nesterov Accelerated Gradient (NAG)** | Improves momentum by calculating gradients with respect to the "lookahead" position | **$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} \mathcal{L}(\theta_t - \mu \cdot v_t) + \mu \cdot v_t$** <br><br> *where:* <br> $v_t$: Velocity at iteration $t$ <br> $\mu$: Momentum factor | - **Pros:** More responsive to changes in gradient direction <br> - **Cons:** Computationally more complex, requires careful tuning |
| **Adagrad**                         | Adapts learning rates for each parameter based on historical gradients | **$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot \nabla_{\theta} \mathcal{L}(\theta_t)$** <br><br> *where:* <br> $G_t$: Diagonal matrix of squared gradients <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Adapts learning rates, no need for manual learning rate adjustment <br> - **Cons:** Learning rate can become too small, leading to slow convergence |
| **RMSprop**                         | Combines ideas from Adagrad and momentum by using a moving average of squared gradients | **$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot \nabla_{\theta} \mathcal{L}(\theta_t)$** <br><br> *where:* <br> $E[g^2]_t$: Exponential moving average of squared gradients <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Well-suited for non-stationary objectives, adapts learning rate <br> - **Cons:** Requires tuning of moving average decay rate |
| **Adam**                            | Combines momentum and RMSprop, using both first and second moment estimates | **$\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$** <br><br> *where:* <br> $\hat{m}_t$: Bias-corrected first moment estimate <br> $\hat{v}_t$: Bias-corrected second moment estimate <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Adaptive learning rates, robust to different types of gradients <br> - **Cons:** Can be less effective in very noisy problems, requires tuning |
| **AdaMax**                          | Variant of Adam using infinity norm for parameter updates | **$\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\hat{v}_t + \epsilon}$** <br><br> *where:* <br> $\hat{m}_t$: Bias-corrected first moment estimate <br> $\hat{v}_t$: Bias-corrected infinity norm estimate <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Can perform better on sparse gradients <br> - **Cons:** Less commonly used, might not always improve over Adam |
| **AdamW**                           | Variant of Adam with weight decay regularization | **$\theta_{t+1} = \theta_t - \eta \cdot \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)$** <br><br> *where:* <br> $\hat{m}_t$: Bias-corrected first moment estimate <br> $\hat{v}_t$: Bias-corrected second moment estimate <br> $\lambda$: Weight decay coefficient <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Reduces overfitting by penalizing large weights, improves generalization <br> - **Cons:** Requires tuning of weight decay parameter |
| **Nadam**                           | Combines NAG and Adam, using momentum terms with adaptive learning rates | **$\theta_{t+1} = \theta_t - \frac{\eta \cdot (\hat{m}_t + \mu \cdot v_t)}{\sqrt{\hat{v}_t} + \epsilon}$** <br><br> *where:* <br> $\hat{m}_t$: Bias-corrected first moment estimate <br> $v_t$: Velocity term from NAG <br> $\hat{v}_t$: Bias-corrected second moment estimate <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Combines advantages of NAG and Adam, improves convergence <br> - **Cons:** More complex, requires tuning of additional parameters |
| **AdaDelta**                        | Extension of Adagrad that reduces the aggressive, monotonically decreasing learning rate problem | **$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \cdot g_t$** <br><br> *where:* <br> $g_t$: Gradient of the loss function <br> $\hat{v}_t$: Running average of past squared gradients <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Robust to large gradients, requires less tuning <br> - **Cons:** More complex than Adagrad |
| **AMSGrad**                         | Variant of Adam that ensures convergence by maintaining a long-term memory of past gradients | **$\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t^{\text{max}}} + \epsilon}$** <br><br> *where:* <br> $\hat{v}_t^{\text{max}}$: Maximum of past squared gradients <br> $\hat{m}_t$: Bias-corrected first moment estimate <br> $\epsilon$: Small constant to prevent division by zero | - **Pros:** Addresses issues with convergence in Adam <br> - **Cons:** Computationally more expensive, less popular |
| **SWATS (Switching from Adam to SGD)** | Starts with Adam for fast convergence, then switches to SGD for better generalization | A combination of Adam and SGD. <br><br> **Phase 1: Adam** <br> $\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ <br> **Phase 2: Transition to SGD** <br> $\eta' = \eta \cdot \frac{1}{\sqrt{\hat{v}_t}}$ <br> $\theta_{t+1} = \theta_t - \eta' \nabla_{\theta} \mathcal{L}(\theta_t)$ | - **Pros:** Balances fast convergence with better generalization <br> - **Cons:** Complex, requires careful switching point determination |
