### PyTorch NLP Layers

| **Layer**                 | **Purpose**                                                    | **Mathematical Formula**                                    | **Key Points**                                                   | **PyTorch Syntax**                     |
|---------------------------|----------------------------------------------------------------|-------------------------------------------------------------|------------------------------------------------------------------|----------------------------------------|
| **Embedding**             | Maps input word indices to dense vector representations (embeddings). | $ \text{Embedding}(x) = W[x] $                          | Transforms sparse word indices into dense vectors. Useful for converting tokens into trainable embeddings. | `torch.nn.Embedding(num_embeddings, embedding_dim)` |
| **RNN**                   | Standard Recurrent Neural Network layer for sequence modeling.  | $ h_t = \sigma(W_h h_{t-1} + W_x x_t + b) $              | Captures sequential dependencies but struggles with long-term dependencies. | `torch.nn.RNN(input_size, hidden_size, num_layers)` |
| **LSTM**                  | Long Short-Term Memory, a type of RNN designed to remember longer sequences. | $ h_t, c_t = \text{LSTM}(x_t, h_{t-1}, c_{t-1}) $        | Solves vanishing gradient problem with gating mechanisms (input, forget, and output gates). | `torch.nn.LSTM(input_size, hidden_size, num_layers)` |
| **GRU (Gated Recurrent Unit)** | Similar to LSTM but with fewer parameters and gates. | $ h_t = \text{GRU}(x_t, h_{t-1}) $                        | Faster than LSTM, combines forget and input gates into a single gate. | `torch.nn.GRU(input_size, hidden_size, num_layers)` |
| **Transformer**           | Self-attention-based architecture for sequence transduction.    | $ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V $ | Captures global dependencies without recurrence; commonly used for NLP tasks like translation. | `torch.nn.Transformer()`               |
| **MultiheadAttention**    | Allows attention mechanism to focus on different positions with multiple attention heads. | $ \text{Multihead}(Q, K, V) = \text{concat}(\text{head}_1, \ldots, \text{head}_h)W^O $ | Enhances attention by attending to information from different representations at different positions. | `torch.nn.MultiheadAttention(embed_dim, num_heads)` |
| **LayerNorm**             | Normalizes input across features for each layer.                | $ \hat{x} = \frac{x - \mu}{\sigma} $                      | Improves training stability in deep networks, commonly used in transformers. | `torch.nn.LayerNorm(normalized_shape)` |
| **PositionwiseFeedForward** | Applies two linear transformations with a ReLU in between, used in Transformers. | $ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $          | Enables learning of complex transformations in each position independently. | (No built-in function; typically done with `torch.nn.Linear` layers and `torch.nn.ReLU`) |
| **PositionalEncoding**    | Adds positional information to input embeddings for transformers. | $ \text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad \text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right) $ | Used in transformers to account for sequence order since they don't have inherent recurrence. | (Custom layer; not built-in, usually added manually) |
| **Linear**                | Fully connected layer for transforming inputs, often used after RNN/Transformer. | $ \text{Linear}(x) = xW + b $                              | Standard linear transformation, often for output layers like classification. | `torch.nn.Linear(in_features, out_features)` |
| **Softmax**               | Converts logits into probabilities for classification tasks.    | $ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} $  | Commonly used in the final layer for multi-class classification problems. | `torch.nn.Softmax(dim)`                |
| **LogSoftmax**            | Softmax followed by logarithm for numerical stability.          | $ \text{LogSoftmax}(x) = \log\left(\frac{e^{x_i}}{\sum_j e^{x_j}}\right) $ | Preferred in loss functions like NLLLoss for better numerical stability. | `torch.nn.LogSoftmax(dim)`             |
| **Dropout**               | Randomly zeroes out elements during training to prevent overfitting. | $ \text{Dropout}(x) = x \cdot \text{Bernoulli}(p) $       | Reduces overfitting by adding regularization. | `torch.nn.Dropout(p)`                  |
| **Bilinear**              | Computes bilinear interactions between two inputs.              | $ \text{Bilinear}(x_1, x_2) = x_1^T W x_2 + b $           | Often used in models where interaction between two inputs is required (e.g., attention mechanisms). | `torch.nn.Bilinear(in1_features, in2_features, out_features)` |
| **Attention**             | Computes weighted average of values based on query-key similarity. | $ \text{Attention}(Q, K, V) = \text{softmax}(QK^T)V $      | Foundation for transformer architectures and useful in tasks that require contextual information. | (No built-in layer; usually implemented manually using matrix multiplication and `torch.nn.Softmax`) |
| **Sequential**            | A container layer to stack multiple layers.                     | -                                                            | Simplifies building models by chaining operations sequentially. | `torch.nn.Sequential(*layers)`          |
